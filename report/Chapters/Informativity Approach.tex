\chapter{Informativity Approach}\doublespacing % Main chapter title
\label{chap:informativityapproach} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}
\section*{Notation}
For the signals $\bmx, \bmu, \bmy$ and $T\in\mathbb{N}$, consider following data matrices
	\begin{itemize}
	\item $X := \bbm x(0) & x(1) & \cdots & x(T)\ebm$
	\item $X_- := \bbm x(0) & x(1) & \cdots & x(T-1) \ebm$
	\item $X_+ := \bbm x(1) & x(2) & \cdots & x(T) \ebm$
	\item $U_- := \bbm u(0) & u(1) & \cdots & u(T-1)\ebm$
	\item $Y_- := \bbm y(0) & y(1) & \cdots & y(T-1)\ebm$
	\end{itemize}
\section{Data informativity framework} \label{sec:prob}
Let's start with few definitions that related to the informativity approach in data-driven control.
\begin{definition}[\cite{vanwaarde2023informativity}, True System]
Denoted by $\calS$, is a mathematical model of the underlying unknown physical system.
\end{definition}
\begin{definition}[\cite{vanwaarde2023informativity}, Model Class]
Denoted by $\mathcal{M}$, is the set of systems that is assumed to contain the `true' system.
\end{definition}
\begin{definition}[\cite{vanwaarde2023informativity}, Data]
Denoted by $\calD$, is the data generated by the true system.
\end{definition}
\begin{definition}[\cite{vanwaarde2023informativity}, Data Consistent Systems]
Denoted by $\Sigma_\calD\subseteq\mathcal{M}$, is the set of all systems in $\calM$ that are consistent with the data $\calD$.
\end{definition}
\begin{definition}[\cite{vanwaarde2023informativity}, Systems with property $\calP$]
Denoted by $\Sigma_\calP\subseteq\mathcal{M}$, is the set of all systems in $\calM$ having the property $\calP$.
\end{definition}
The key idea of the informativity approach is that the true system satisfies a property $\calP$ if \emph{all} the systems that are consistent with the data satisfy that property (\ref{fig:informativedata} and \ref{fig:notinformativedata}).
% The idea is that Since the only information we have to base our answer on are the data $\calD$ obtained from the true system, we can only conclude from the data that the true system has property $\calP$ if \textit{all} systems consistent with the data $\calD$ have the property $\calP$. If this is the case, we call the data informative for the system property. This leads to the following definition, see also F 
\begin{figure}[h!]
		\centering
			\begin{tikzpicture}[scale=1]
			\node[draw,rectangle,minimum width=8cm,minimum height = 4cm] (1) at (0,0) {};
			\node[] (2) at (3.6,1.7) {$\mathcal{M}$};
			\node[draw,fill,style=circle,inner sep=0pt,minimum size=2pt,color=magenta] (3) at (-2.5,-1) {};
			\node[color=magenta] (4) at (-2.7,-1) {$\mathcal{S}$};
			\node[label=right:{$\mathcal{M}$: model class}] at (-4,-2.7) {};
			\node[label=right:{\magenta{$\mathcal{S}$: unknown system}}] at (-4,-3.2) {};
			\node[label=right:{$\mathcal{D}$: given data set}] at (-4,-3.7) {};
			\node[label=right:{\green{$\Sigma_\mathcal{D}$: data consistent systems}}] at (-0.3,-2.7) {};
			\node[label=right:{\gray{$\Sigma_\mathcal{P}$: systems with property $\calP$}}] at (-0.3,-3.7) {};
			\node[label=right:{$\mathcal{P}$: system property}] at (-0.3,-3.2) {};
			\draw [fill=greenpigment,fill opacity=0.3] (-1.3,-0.8) ellipse (1.8cm and 1cm);
			\draw [fill=gray,fill opacity=0.3] (-1,-0.4) ellipse (2.5cm and 1.5cm);
			\node[] at (-1,.4) {\green{$\Sigma_\mathcal{D}$}};
			\node[] at (0.1,1.2) {\gray{$\Sigma_\mathcal{P}$}};
			%{$\mathcal{P}$}
		\end{tikzpicture}
		\caption{\cite{vanwaarde2023informativity}, Data is informative for property}
		\label{fig:informativedata}
\end{figure}	
\begin{figure}[h!]
		\centering
			\begin{tikzpicture}[scale=1]
			\node[draw,rectangle,minimum width=8cm,minimum height = 4cm] (1) at (0,0) {};
			\node[] (2) at (3.6,1.7) {$\mathcal{M}$};
			\node[draw,fill,style=circle,inner sep=0pt,minimum size=2pt,color=magenta] (3) at (-2.5,-1) {};
			\node[color=magenta] (4) at (-2.7,-1) {$\mathcal{S}$};
			\draw [fill=greenpigment,fill opacity=0.3] (-1.3,-0.8) ellipse (1.8cm and 1cm);
			\draw [fill=gray,fill opacity=0.3] (0.5,-0.4) ellipse (2.5cm and 1.5cm);
			\node[] at (-1,.4) {\green{$\Sigma_\mathcal{D}$}};
			\node[] at (1.6,1.2) {\gray{$\Sigma_\mathcal{P}$}};
			%{$\mathcal{P}$}
		\end{tikzpicture}
		\caption{\cite{vanwaarde2023informativity}, Data is not informative for property}
		\label{fig:notinformativedata}
\end{figure}
\begin{definition}[\cite{vanwaarde2023informativity}, Informativity for analysis]\label{ch1:def:informativity}
	The data $\calD$ is \textit{informative} for property $\calP$ if $\Sigma_\calD \subseteq\Sigma_\calP$.
\end{definition}	
\begin{problem}[\cite{vanwaarde2023informativity}, Informativity problem for analysis]\label{ch3:prob:general}
	Provide necessary and sufficient conditions on the data $\calD$ under which these data are informative for property $\calP$. 
\end{problem}
\subsection{Incorporating Control problems}
Consider the problem of designing a controller $\calK$ meeting a desired control objective $\cal{O}$ for a data-driven control problem. Let $\Sigma_{\calO}$ be the set of all systems satisfying the control objective $\calO$ and $\Sigma_{\calD}(\calK)$ are all systems obtained by interconnecting the systems in $\Sigma_{\calD}$ with the controller. This give rise to the following informativity problem.
% For the framework to allow for data-driven control problems, we will consider a given control objective 
% $\cal{O}$.  Denote by  the set of all systems that satisfy the control objective $\calO$. For a given controller $\calK$, denote by $\Sigma_{\calD}(\calK)$ the set of all systems obtained as the interconnection of a system in $\Sigma_{\calD}$ with the controller $\calK$. We then have the following variant of informativity:
\begin{definition}[\cite{vanwaarde2023informativity}, Informativity for control]\label{ch1:def:par informativity}
The data $\calD$ is \textit{informative} for the control objective $\calO$ if there exists a controller $\calK$ such that $\Sigma_{\calD}(\calK) \subseteq \Sigma_{\calO}$. 
\end{definition}
And the two problems associated with this informativity is to first determine if such controller exists and then to design the controller.
% Obviously, the first step in any data-driven control problem is to determine whether it is possible to obtain, from the given data, a suitable controller. This leads to the following informativity problem:
\begin{problem}[\cite{vanwaarde2023informativity}, Informativity problem for control]\label{ch1:prob:parametrized}
Provide necessary and sufficient conditions on $\calD$ under which the data are informative for the control objective $\calO$. 
\end{problem}
% The second step of data-driven control involves the design of a suitable controller. In terms of our framework, this can be stated as:
\begin{problem}[\cite{vanwaarde2023informativity}, Control design problem]\label{ch1:prob:design}
	Assuming the data $\calD$ is informative for the control objective $\calO$, find a controller $\calK$ such that $\Sigma_\calD (\calK) \subseteq \Sigma_{\calO}$. 
\end{problem}
% The data-driven control design problem as formulated here has a rather natural interpretation as a problem of robust control. Indeed, the aim is to find one single controller that achieves the design objective for all systems  that are consistent with the data. In other words, the `system uncertainty` is determined directly by the given data, and no attempt is made to identify in any sense an uncertainty description that is suitable for existing methods in robust control design. The given data are called informative for a given design objective if the associated robust control problem allows a solution for the system uncertainty imposed by the data.

This framework is already used for various control problems taking two main directions, based on exact data (`E') and noisy (`N') data and based on data requirements such as the need of input (`I'), output (`O') and state (`S') data.
Table~\ref{tab:summary} provides an summary of the available results and their data requirements.

% The data informativity framework has already been applied to various analysis and design problems. Generally speaking, there is a dichotomy between two main directions. On the one hand there are analysis and design problems based on exact, i.e., noiseless data. On the other hand we consider the more realistic situation that the data are noisy, in the sense that they are obtained from a true, unknown, system that is corrupted by additive noise. 

% The first column of the table states the considered system property or control design problem. The second column refers to the type of data that are used (for discrete-time systems). Here, `E' refers to exact data, and `N' to noisy data. State, input-state, input-state-output and input-output are denoted by `S', `IS', `ISO' and `IO', respectively.
% The results in the table apply to , and most of the results are for linear time-invariant dynamics.
\begin{table}[H]  
\begin{center}
{\small\begin{tabular}{l|c}
Problem & Data\\
\hline
{controllability} & E-IS\\
observability & E-S\\
{stabilizability} & E-IS, N-IS\\
stability & E-S, N-S, N-IO\\
{LQR} & E-IS\\
{dissipativity} & E-ISO, N-ISO\\
{tracking and regulation} & E-IS\\
\end{tabular}}
\end{center}
\caption{\cite{vanwaarde2023informativity}, Summary of results}
\label{tab:summary}
\end{table}
Initially we will review select analysis and design problems and then proceed to describe a new approach based on current literature.
% The purpose of this paper is to highlight the strength of the informativity framework by reviewing a selection of analysis and design problems, indicated in Table~\ref{tab:summary} in red. Both exact and noisy data will be discussed in the present paper.

% This paper is divided into three main sections. These sections are divided into subsections, each devoted to a particular analysis or control design problem. In the first main section our model class will be chosen as the set of all discrete-time linear input-state systems with given state and input dimensions. The data are measurements of the state and input obtained from a true, unknown, system on a given finite time-interval. In this first section it is assumed that the data are noiseless, in the sense that the true system does not contain any noise input. In this noiseless framework we discuss the problem of informativity for the system properties controllability and stabilizability. Next, as a first control design problem we discuss the problem of stabilization by static state feedback, and take a look at the corresponding informativity problem. In the third subsection we study informativity in the context of the linear quadratic regulator problem, and, finally, in the fourth subsection we look at the classical problem of tracking and regulation. 

% In the second main section we incorporate noise into the models and data. The model class consists of all input-state systems with additive noise and the input-state data are assumed to be obtained from the noisy true system. The additive noise is unknown, but its samples on the data sampling interval are assumed to satisfy a given quadratic matrix inequality. In this framework we discuss a number of analysis and control design problems. In the first subsection we again look at the problem of stabilization by state feedback, this time in a noisy setting. The next subsection deal with informativity in the context of the well known ${\calH}_{\infty}$ control problem. The final subsection of this part deals with the problem of determining from noisy data whether an unknown system is dissipative with respect to a given supply rate.
\section{Informativity for analysis (Controllability and Stabilizability)}
% Recall from Definition~\ref{ch1:def:informativity} the definition of informativity of data for a given system property. In accordance with Definition~\ref{ch1:def:informativity} we say that the data $(U_-,X)$ are {\em informative for controllability} if all systems in $\Sigma_\calD$ are controllable. Likewise, we call the data  {\em informative for stabilizability} if all systems in $\Sigma_\calD$ are stabilizable.
The informativity notion is straightforward from the definition~\ref{ch1:def:informativity}. Given data is {\em informative for controllability (stabilizability)} if all systems in $\Sigma_\calD$ are controllable (stabilizable).

So, let's focus on the informativity problem for analysis which can be solved using the following theorem.
\begin{theorem}[\cite{vanwaarde2023informativity}, Data-driven Hautus tests]
	\label{ch2:t:contstab}
	The data $(U_-,X)$ is informative for controllability (stabilizability) if and only if 
	\begin{equation}
		\label{ch2:eq:rank for cont}
		\rank ( X_+ -\lambda X_-) = n \quad \forall \lambda \in \mathbb{C} \quad (\textrm{with } |\lambda|\geq 1).
	\end{equation}
	% Similarly, the data $(U_-,X)$ are informative for  if and only if 
	% \begin{equation}\label{ch2:eq:rank for stab}
	% 	\rank ( X_+ -\lambda X_-) = n \quad \forall \lambda \in \mathbb{C}\textrm{ with } |\lambda|\geq 1.
	% \end{equation}
\end{theorem}
This also shows that the rank condition \eqref{ch2:eq:inf for sys ident} is not necessary for data-driven analysis.
% Note that condition \eqref{ch2:eq:inf for sys ident} is not necessary to perform data-driven analysis in general. There are situations in which we can conclude controllability or stabilizability from the data without being able to identify the true system uniquely. This can be seen by the fact that in order for \eqref{ch2:eq:inf for sys ident} to hold we require at least $m+n$ separate measurements, whereas the conditions of Theorem~\ref{ch2:t:contstab} can hold for $n$ measurements. This is illustrated in the following example.

\begin{comment}
	\begin{exm}[Full rank is not necessary]
		Suppose that $n=2$ and $m=1$. Assume we collect data on the single time interval $\{0,1 \ldots, T\}$ with $T = 2$ to obtain
		\[ X= \begin{bmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} \textrm{ and } U_-= \begin{bmatrix} 1 & 0 \end{bmatrix}. \] 
		This implies that 
		\[X_+ = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \textrm{ and } X_- = \begin{bmatrix} 0 & 1 \\ 0& 0\end{bmatrix}. \]
		Using Theorem~\ref{ch2:t:contstab} we see that these data are informative for controllability, as 
		\[ \rank \begin{bmatrix} 1 & -\lambda \\ 0 & 1 \end{bmatrix}= 2 \quad \forall \lambda \in \mathbb{C}.\] 
		Recall that this means that all systems consistent with the data are controllable. Therefore we can conclude that also the true system is controllable. Moreover, note that the rank condition \eqref{ch2:eq:inf for sys ident} does not hold, and therefore $\Sigma_\calD$ is not a singleton.  To be precise:
		\begin{equation}\label{ch2:eq:structural} \Sigma_\calD = \left\lbrace \left( \begin{bmatrix} 0 & a_1 \\ 1 & a_2\end{bmatrix} ,\begin{bmatrix} 1 \\ 0 \end{bmatrix}\right) \mid a_1,a_2 \in\mathbb{R} \right\rbrace.
		\end{equation}
		This means that there are multiple systems consistent with the data.
	\end{exm}
\end{comment}
% Computationally, the conditions \eqref{ch2:eq:rank for cont} and \eqref{ch2:eq:rank for stab} might seem daunting, since these require to test the rank of a matrix for each $\lambda\in\mathbb{C}$. However, it is well known that in order for the classical Hautus test to be satisfied, it suffices to test the rank for only $\lambda\in \sigma(A)$, where $\sigma(A)$ denotes the set of eigenvalues of the matrix $A$. 
\begin{comment}
	
	In a similar fashion, the conditions of Theorem~\ref{ch2:t:contstab} can be verified in a finite number of steps. Indeed, \eqref{ch2:eq:rank for cont} is equivalent to 
	\[ \rank(X_+)=n \textrm{ and } \rank ( X_+ -\lambda X_-) = n \]
	for all $\lambda\neq 0$ with $\lambda\inv\in\sigma(X_-X_+^\sharp)$, where $X_+^\sharp$ is any right inverse of $X_+$. Regarding stabilizability, we obtain that \eqref{ch2:eq:rank for stab} is equivalent to 
	\[\rank(X_+-X_-)=n \textrm { and } \rank ( X_+ -\lambda X_-) = n \]
	for all $\lambda\neq 1$ with $(\lambda-1)\inv\in\sigma(X_-(X_+-X_-)^\sharp)$.

	%\begin{remark}[Comparing this to the other results on controllability]
	%	- Harry/Jaap paper? The paper(s) by Liu? 
	%\end{remark}
\end{comment}
% After considering data-driven controllability and stabilizability analysis in the previous subsection, we now turn attention to data-driven control design. In particular, we consider the quintessential control problem of stabilization by state feedback. 
\section{Informativity for control (Stabilization)}
\label{sec:stability informativity}
Usingn~\ref{ch1:def:par informativity}. 
We define our sets as follows,
% We take the model class $\calM$ and data $\calD$ as before, and take as the control objective $\calO$: `interconnection with a  state feedback controller yields a stable\footnote{meaning Schur, that is, all its eigenvalues $\lambda$ satisfy $\abs{\lambda}<1$.}, closed loop system'. This means that the set $\Sigma_{\calO}$ of all systems that satisfy the control objective is equal to the set of all stable $n \times n$ matrices
$$
\Sigma_{\calO} :=  \{ A \in \R^{n \times n} \mid A \text{ is stable} \} \qquad\text{and}\qquad \Sigma_\calD(K) = \{ A+ BK \mid (A,B) \in \Sigma_\calD \}. 
$$

As per \ref{ch1:def:par informativity}, data $(U_-,X)$ is {\em informative for stabilization by state feedback} if there exists a $K \in \mathbb{R}^{m \times n}$ such that $\Sigma_\calD(K) \subseteq \Sigma_{\calO}$.

% In other words, the input-state data $(U_-,X)$ are informative for stabilization by state feedback if there exists a single real $m \times n$ matrix $K$ such that $A +BK$ is stable for all $(A,B) \in \calM$ that are consistent with the data.

%\begin{remark} 
% At this point, one may wonder about the relation between informativity for stabilizability and informativity for stabilization. It is clear that the data $(U_-,X)$ are informative for stabilizability if $(U_-,X)$ are informative for stabilization by state feedback. However, the reverse statement does not hold in general. This is due to the fact that all systems $(A,B)$ in $\Sigma_\calD$ may be stabilizable, but there may not exist a \emph{common} feedback gain $K$ such that $A+BK$ is stable for all of these systems. 
%\end{remark}
\begin{comment}
	
	The following example further illustrates the difference between informativity for stabilizability and informativity for stabilization.

	\begin{exm}[Stabilizability and stabilization]
		Consider the scalar system 
		$
		\bmx(t+1)= \bmu(t),
		$
		where $\bmx, \bmu \in \mathbb{R}$. Suppose that we collect data on the single time interval $\{0,1\}$, specifically, $x(0) = 0$, $u(0) = 1$ and $x(1) = 1$. This means that $U_- = \begin{bmatrix}1 \end{bmatrix}$ and $X = \begin{bmatrix}0 & 1	\end{bmatrix}$. It can be shown that $\Sigma_\calD = \{(a,1) \mid a \in \mathbb{R}\}$. Clearly, all systems in $\Sigma_\calD$ are stabilizable. Nonetheless, the data are not informative for \emph{stabilization}. This is because the systems $(-1,1)$ and $(1,1)$ in $\Sigma_\calD$ cannot be stabilized by the \emph{same} controller of the form $u(t) = K x(t)$. 
		We conclude that informativity of the data for stabilizability does not imply informativity for stabilization by state feedback. 
	\end{exm}

	Having defined the notion of informativity for stabilization, we now take the steps described in the introduction. First, we resolve Problem~\ref{ch1:prob:parametrized}, that is, we find necessary and sufficient conditions for informativity for stabilization by state feedback. After this, we design a corresponding controller, as described in Problem~\ref{ch1:prob:design}. 

\end{comment}
To solve the problem, consider the solution set of the homogeneous equation which will allow the to deduce the membership of $A,B$ in $\Sigma_{\calD}$
\begin{equation}\label{ch3:eq:sigma is 0}
	\Sigma_\calD^0 := \left\{(A_0,B_0) \mid 0 = \bbm A_0 & B_0 \ebm
	\begin{bmatrix}
		X_- \\ U_-
	\end{bmatrix}\right\}.
\end{equation}
We first state a useful lemma using the fact that $(A,B)\in\Sigma_\calD$ if and only if it is a solution of the the corresponding affine equation. Now let $\Sigma_\calD^0$ denote the solution set of the corresponding homogeneous equation. That is, 
% This allows us to state the following lemma. 
\begin{lemma}[\cite{vanwaarde2023informativity}, A necessary condition]
	\label{ch3:lemmaF0=0}
	If the data $(U_-,X)$ is informative for stabilization by state feedback then $A_0 + B_0 K = 0$ for all $(A_0,B_0) \in \Sigma_\calD^0$. Equivalently, 
	$$
	\im \begin{bmatrix}
		I \\ K
	\end{bmatrix} \subseteq 
	\im \begin{bmatrix}
		X_- \\ U_-
	\end{bmatrix}.
	$$
\end{lemma}

% The solution set of an affine equation is equal to the sum of any solution and the solution set of the corresponding homogenous equation. Since we know that $(A_s,B_s)\in\Sigma_\calD$ by definition, this means that we can write 
% \[ \Sigma_\calD = (A_s,B_s) + \Sigma_\calD^0.\]
% Using this, as a consequence of Lemma~\ref{ch3:lemmaF0=0} we have that if $K$ is a  feedback gain such that $A+BK$ is stable for all $(A,B) \in \Sigma_\calD$, then
% \[ \Sigma_\calD(K) = \{A_s+B_sK\},  \] 
% that is, the set of closed-loop systems consistent with the data is a singleton. It is important to note, however, that this does not mean that $\Sigma_\calD$ is necessarily a singleton.

% The above observation turns out to be instrumental in proving the following theorem, which gives necessary and sufficient conditions for informativity for stabilization by state feedback. 

This observations helps us to solve the problem of informativity for stabilization.
\begin{theorem}[\cite{vanwaarde2023informativity}, Conditions for stabilization]
	\label{ch3:t:algstab}
	The data $(U_-,X)$ is informative for stabilization by state feedback if and only if the matrix $X_-$ has full row rank and there exists a right inverse $X_-^\sharp$ of $X_-$ such that $X_+ X_-^\sharp$ is stable. 
	
	Moreover, $K$ is such that $A +BK$ is stable for all $(A,B) \in \Sigma_\calD$ if and only if $K = U_- X_-^\sharp$.
	% , where $X_-^\sharp$ satisfies the above properties.
	% In that case, $A +BK = X_+ X_-^\sharp$ for all $(A,B) \in \Sigma_\calD$.
\end{theorem}
The above theorem though is not constructive as it does not provide the means to calculate such right inverse of $X_-$. To resolve this, the following LMI-based approach is utilised.
% Theorem \ref{ch3:t:algstab} gives a characterization of all input-state data that are informative for stabilization by state feedback and provides a stabilizing controller. Nonetheless, the procedure to compute this controller might not be entirely satisfactory since it is not clear how to find a right inverse of $X_-$ that makes $X_+ X_-^\sharp$ stable. In general, $X_-$ has many right inverses, and $X_+ X_-^\sharp$ can be stable or unstable depending on the particular right inverse $X_-^\sharp$. To deal with this problem and to solve the design problem, we give a characterization of informativity for stabilization in terms of linear matrix inequalities (LMIs). The feasibility of such LMIs can be verified using standard tools. 
\begin{theorem}[\cite{vanwaarde2023informativity}, LMI conditions for stabilization]\label{ch3:t:lmistab}
	The data $(U_-,X)$ are informative for stabilization by state feedback if and only if there exists a matrix $\Theta \in \mathbb{R}^{T \times n}$ satisfying
	\begin{equation}
		\label{ch3:LMI/E}
		X_- \Theta = (X_- \Theta)^\top\quad\text{ and }\quad
		\begin{bmatrix}
			X_- \Theta & X_+ \Theta \\ \Theta^\top X_+^\top & X_- \Theta
		\end{bmatrix} > 0.
	\end{equation}
	Moreover, $K$ is such that $A +BK$ is stable for all $(A,B) \in \Sigma_\calD$ if and only if $K = U_- \Theta (X_-\Theta)^{-1}$ for some matrix $\Theta$ satisfying \eqref{ch3:LMI/E}.
\end{theorem}
\begin{comment}
	
	The following example provides a simple illustration of the above results. 
	\begin{exm}[Full rank not necessary for informativity] 
		Consider an unstable system $(A_s,B_s)$, where $A_s$ and $B_s$ are given by
		$$
		A_s = \begin{bmatrix}
			1.5 & 0 \\ 1 & 0.5
		\end{bmatrix}, \quad B_s = \begin{bmatrix}
			1 \\ 0
		\end{bmatrix}.
		$$
		We collect data from this system on a single time interval from $t = 0$ until $t = 2$, which results in the data matrices
		$$
		X = \begin{bmatrix}
			1 & 0.5 & -0.25 \\
			0 & 1 & 1
		\end{bmatrix}, \quad U_- = \begin{bmatrix}
			-1 & -1
		\end{bmatrix}.
		$$
		Clearly, the matrix $X_-$ is square and invertible, and it can be verified that
		$$
		X_+ X_-^{-1} = \begin{bmatrix}
			0.5 & -0.5 \\
			1 & 0.5
		\end{bmatrix}
		$$
		is stable, since its eigenvalues are $\half(1 \pm \sqrt{2}i)$. We conclude by Theorem \ref{ch3:t:algstab} that the data $(U_-,X)$ are informative for stabilization by state feedback. The same conclusion can be drawn from Theorem \ref{ch3:t:lmistab} since $$\Theta = \begin{bmatrix}
			1 & -1 \\ 0 & 2
		\end{bmatrix}$$ solves \eqref{ch3:LMI/E}. Next, we can conclude from either Theorem \ref{ch3:t:algstab} or Theorem \ref{ch3:t:lmistab} that the stabilizing feedback gain in this example is unique, and given by $K = U_-X_-^{-1} = \begin{bmatrix}
			-1 & -0.5
		\end{bmatrix}$. Finally, it is worth noting that the data are not informative for identification. In fact, $(A,B) \in \Sigma_\calD$ if and only if
		$$
		A = \begin{bmatrix}
			1.5+a_1 & 0.5a_1 \\ 1+ a_2 & 0.5+ 0.5a_2
		\end{bmatrix}, \quad B = \begin{bmatrix}
			1+a_1 \\ a_2
		\end{bmatrix}
		$$
		for some $a_1$ and $a_2 \in \mathbb{R}$.
	\end{exm}
\end{comment}
%
%
%\section{Analysis and control using exact input-state data}
%
%In order to provide a solid foundation for more complex problems, in this section we will first consider the essential model class of linear, time-invariant input-state systems. Our goal is to analyze and control these systems on the basis of input-state data, consisting of a finite number of measurements of the input and state trajectories. Moreover, we will assume that these measurements are exact, that is, not corrupted by any noise. 
%
%Given this situation, we can make the abstract framework that was introduced in the introduction more tangible. To be precise, the unknown system $\calS$ is assumed to be the following:
%\begin{equation} \label{ch2:eq:true system once more}
%	\bmx(t+1) = A_s\bmx(t) + B_s\bmu(t),
%\end{equation}
%where $\bmx$ denotes the $n$-dimensional state and $\bmu$ the $m$-dimensional input. In the following, we assume that the dimensions $n$ and $m$ are known, but the matrices $A_s$ and $B_s$ are unknown. As such, we see that $\calS$ is contained in the model class $\calM$ given by the set of all discrete-time linear input-state systems of the form
%\begin{equation}  \label{ch2:e: is-system}
%	\bmx(t+1) = A\bmx(t) + B\bmu(t),
%\end{equation}
%with given state space and input dimensions $n$ and $m$. 
%
%Suppose that we collect input-state data from the true system \eqref{ch2:eq:true system once more} on $\ell$ time intervals $\pset{0,1,\ldots,T_i}$ for $i=1,2,\ldots,\ell$, in the sense that we excite the true system with input sequences $u^i(0)$, $u^i(1)$, $\ldots$, $u^i(T_i-1)$ and obtain measurements of corresponding state sequences $x^i(0)$, $x^i(1)$, $\ldots$, $x^i(T_i)$. We can collect these measurements in matrices by defining for each $i$:
%\begin{subequations}\label{ch2:eq: UXdata}
%	\begin{align}
%		U^i_-& := \bbm u^i(0) & u^i(1) & \cdots & u^i(T_i-1)\ebm, \\
%		X^i& := \bbm x^i(0) & x^i(1) & \cdots & x^i(T_i)\ebm. \label{ch2:eq: UXdata2}
%	\end{align}
%\end{subequations}
%If, additionally, we define the matrices
%\begin{subequations}\label{ch2:eq: def of X- X+}
%	\begin{align}
%		X^i_-& := \bbm x^i(0) & x^i(1) & \cdots & x^i(T_i-1) \ebm, \\
%		X^i_+& := \bbm x^i(1) & x^i(2) & \cdots & x^i(T_i) \ebm,
%	\end{align}	
%\end{subequations}
%we have $X^i_+=A_sX^i_-+B_sU^i_-$ for each $i$, since the data were assumed to be generated by the true system. In a similar fashion, we can arrange these matrices corresponding to separate intervals in the following manner:
%\bse\label{ch2:eq: UXdatanew}
%\begin{alignat}{3}
%	U_-&:=\bbm U^1_-&\cdots& U^\ell_-\ebm,&\quad X&:=\bbm X^1&\cdots& X^\ell\ebm,\label{ch2:eq: UXdatanew1}\\
%	X_-&:=\bbm X^1_-&\cdots& X^\ell_-\ebm,&\quad X_+&:=\bbm X^1_+&\cdots& X^\ell_+\ebm,\label{ch2:eq: UXdatanew2}
%\end{alignat}
%\ese
%As before, it is straightforward to check that $X_+=A_sX_-+B_sU_-$. Moreover, this is \textit{all} the information we have regarding the true system on the basis of the data $\calD:=(U_-,X)$. As explained in the introduction, we are interested in the set $\Sigma_\calD$ containing all systems in $\calM$ that are consistent with these data. This means that the set $\Sigma_\calD$ is equal to $\Sigma_{(U_-,X)}$, defined by 
%\begin{equation}\label{ch2:eq:SigmaD}
%	\Sigma_{(U_-,X)} := \left\{ (A,B) \in \calM \mid X_+= \bbm A&B \ebm
%	\begin{bmatrix}
%		X_-\\U_-
%	\end{bmatrix} \right\}. 
%\end{equation}
%
%Note that since $(A_s,B_s) \in \Sigma_{(U_-,X)}$, it is nonempty. Moreover it is the solution set of an affine equation. Therefore, it is a singleton if and only if 
%\begin{equation}\label{ch2:eq:inf for sys ident} \rank \begin{bmatrix} X_- \\ U_- \end{bmatrix} = n+m. \end{equation}
%
%Under this condition, we can find $\Sigma_{(U_-,X)}=\{(A_s,B_s)\}$, and then use a model-based method to check whether $(A_s,B_s)$ has a given property. 
%
%\begin{remark}[The fundamental lemma] 
%	In the previous, we have considered the situation where the measurements are given. A problem that is closely related to those considered in this paper is: Can we pick inputs $u(t)$, such that the resulting state trajectory is such that \eqref{ch2:eq:inf for sys ident} holds? The answer to this question can be  found using the notion of \textit{persistently exciting} inputs. 
%	
%	To be precise, consider the finite length sequence $u(0)$, $u(1)$, $\ldots$, $u(T-1)$. Denote the vector
%	\[ u_{[i,j]} = \begin{bmatrix} u(i)^\top \ldots u(j)^\top\end{bmatrix}^\top, \]
%	and define the Hankel matrix of depth $k$ by
%	
%	\[H_k(u_{[0,T-1]}) := \begin{bmatrix}
%		u(0) 	& u(1) 		& \cdots 	& u(T - k) \\
%		u(1) 	& u(2) 		& \cdots 	& u(T - k +1) \\
%		\vdots  & \vdots	&    		& \vdots  \\
%		u(k-1)  & u(k) 		& \cdots  	& u(T-1) 
%	\end{bmatrix}. \]
%	We say that the input sequence $u_{[0,T-1]}$ is \textit{persistently exciting} of order $k$ if $H_k(u_{[0,T-1]})$ has full row rank. 
%	
%	A special case of Willem's fundamental lemma \cite[Thm. 1]{vanWaarde2020c} (originally proven in a behavioral context in \cite[Thm. 1]{Willems2005}) states the following: Suppose that the measured system is controllable and that the input sequence is \textit{persistently exciting} of order $n+L$. Then any  $\bar{u}_{[0,L-1]},\bar{x}_{[0,L-1]}$ is an input/state trajectory of the system if and only if 
%	\[ \begin{bmatrix} \bar{u}_{[0,L-1]} \\ \bar{x}_{[0,L-1]} \end{bmatrix} \in \im \begin{bmatrix} H_L(u_{[0,T-1]}) \\ H_L(x_{[0,T-1]}) \end{bmatrix}. \] 
%	
%	As a consequence, we can guarantee that \eqref{ch2:eq:inf for sys ident} holds by assuming that $u_{[0,T-1]}$ is persistently exciting of degree $n+1$. Recall that, if the condition \eqref{ch2:eq:inf for sys ident} holds, we know that there exists a unique system in $\Sigma_{(U_-,X)}$. Once we have identified a model of this system, we can apply any number of model-based methods in order to, for instance, stabilize it. 
%	
%	However, in many applications a (state space) model might not be the most convenient form to work with. One of these applications is Data-enabled Predictive control (DeePC), introduced in \cite{Coulson2019}. As a data-driven variant of Model Predictive Control (MPC), the central problem is to minimize a cost function over all length-$L$ trajectories. The fundamental lemma allows this to be formulated in terms of measurements, without explicitly finding a system model.  
%	
%	On the other hand, there are many other applications where avoiding the modeling step, and dealing with data directly is convenient. For example, the paper \cite{DePersis2020} provides efficient methods for stabilization on the basis measurements for which \eqref{ch2:eq:inf for sys ident} holds. 	
%\end{remark}
%
%\subsection{Controllability and stabilizability}
%
%As we will show in this subsection, condition \eqref{ch2:eq:inf for sys ident} is not necessary to perform data-driven analysis in general. As an illustration, we will establish necessary and sufficient conditions in terms of the data for verifying controllability and stabilizability, which do not require the rank condition to hold.
%
%Recall from Definition~\ref{ch1:def:informativity} the definition of informativity of data for a given system property. In accordance with Definition~\ref{ch1:def:informativity} we say that the data $(U_-,X)$ are {\em informative for controllability} if all systems in $\Sigma_{(U_-,X)}$ are controllable. Likewise, we call the data  {\em informative for stabilizability} if all systems in $\Sigma_{(U_-,X)}$ are stabilizable.
%
%In order to establish tests for these notions of informativity, the well known Hautus test 
%%\cite[Thm. 3.13]{Trentelman2001} 
%for controllability can be used: a system $(A,B)$ is controllable if and only if
%\begin{equation}\label{ch2:eq:Hautus} \rank \begin{bmatrix} A-\lambda I & B \end{bmatrix} =n \end{equation} 
%for all $\lambda\in \mathbb{C}$. For stabilizability, the Hautus test requires that \eqref{ch2:eq:Hautus} holds for all $\lambda$ outside the open unit disc.
%
%%Now, introduce the following subsets of the model class $\calM$ of all systems of the form \eqref{ch2:e: is-system} with fixed dimensions $n$ and $m$:
%%\begin{align*}
%%\Sigma_{\text{cont}} &:= \{ (A,B) \mid (A,B) \text{ is controllable} \} \\
%%\Sigma_{\text{stab}} &:= \{ (A,B) \mid (A,B) \text{ is stabilizable} \}.
%%\end{align*} 
%
%The following theorem gives necessary and sufficient conditions on the input-state data to be informative for these two properties. The result provides tests on the given data matrices
%
%
%\begin{theorem}[Data-driven Hautus tests]
%	\label{ch2:t:contstab}
%	The data $(U_-,X)$ are informative for controllability if and only if 
%	\begin{equation}
%		\label{ch2:eq:rank for cont}
%		\rank ( X_+ -\lambda X_-) = n \quad \forall \lambda \in \mathbb{C}.
%	\end{equation}
%	Similarly, the data $(U_-,X)$ are informative for stabilizability if and only if 
%	\begin{equation}\label{ch2:eq:rank for stab}
%		\rank ( X_+ -\lambda X_-) = n \quad \forall \lambda \in \mathbb{C}\textrm{ with } |\lambda|\geq 1.
%	\end{equation}
%\end{theorem}
%
%As announced at the beginning of this section, there are situations in which we can conclude controllability/stabilizability from the data without being able to identify the true system uniquely. This can be seen by the fact that in order for \eqref{ch2:eq:inf for sys ident} to hold we require at least $m+n$ separate measurements, whereas the conditions of Theorem~\ref{ch2:t:contstab} can hold for $n$ measurements. This is illustrated in the following example.
%
%\begin{exm}
%	Suppose that $n=2$ and $m=1$. Assume we collect data on the single time interval $\{0,1 \ldots, T\}$ with $T = 2$ to obtain
%	\[ X= \begin{bmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} \textrm{ and } U_-= \begin{bmatrix} 1 & 0 \end{bmatrix}. \] 
%	This implies that 
%	\[X_+ = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \textrm{ and } X_- = \begin{bmatrix} 0 & 1 \\ 0& 0\end{bmatrix}. \]
%	Using Theorem~\ref{ch2:t:contstab} we see that these data are informative for controllability, as 
%	\[ \rank \begin{bmatrix} 1 & -\lambda \\ 0 & 1 \end{bmatrix}= 2 \quad \forall \lambda \in \mathbb{C}.\] 
%	Recall that this means that all systems consistent with the data are controllable. Therefore we can conclude that also the true system is controllable. Moreover, note that the rank condition \eqref{ch2:eq:inf for sys ident} does not hold, and therefore $\Sigma_{(U_-,X)}$ is not a singleton.  To be precise:
%	\begin{equation}\label{ch2:eq:structural} \Sigma_{(U_-,X)} = \left\lbrace \left( \begin{bmatrix} 0 & a_1 \\ 1 & a_2\end{bmatrix} ,\begin{bmatrix} 1 \\ 0 \end{bmatrix}\right) \mid a_1,a_2 \in\mathbb{R} \right\rbrace.
%	\end{equation}
%	This means that there are multiple systems consistent with the data.
%\end{exm}
%
%Computationally, the conditions \eqref{ch2:eq:rank for cont} and \eqref{ch2:eq:rank for stab} might seem daunting, since these require to test the rank of a matrix for each $\lambda\in\mathbb{C}$. However, it is well known that in order for the classical Hautus test to be satisfied, it suffices to test the rank of \eqref{ch2:eq:Hautus} for only $\lambda\in \sigma(A)$, where $\sigma(A)$ denotes the set of eigenvalues of the matrix $A$. 
%
%In a similar fashion, the conditions of Theorem~\ref{ch2:t:contstab} can be verified in a finite number of steps. Indeed, \eqref{ch2:eq:rank for cont} is equivalent to 
%\[ \rank(X_+)=n \textrm{ and } \rank ( X_+ -\lambda X_-) = n \]
%for all $\lambda\neq 0$ with $\lambda\inv\in\sigma(X_-X_+^\sharp)$, where $X_+^\sharp$ is any right inverse of $X_+$. Regarding stabilizability, we obtain that \eqref{ch2:eq:rank for stab} is equivalent to 
%\[\rank(X_+-X_-)=n \textrm { and } \rank ( X_+ -\lambda X_-) = n \]
%for all $\lambda\neq 1$ with $(\lambda-1)\inv\in\sigma(X_-(X_+-X_-)^\sharp)$.
%
%%\begin{remark}[Comparing this to the other results on controllability]
%%	- Harry/Jaap paper? The paper(s) by Liu? 
%%\end{remark}
%
%
%\subsection{Stabilization}
%After considering data-driven controllability and stabilizability analysis in the previous subsection, we now turn attention to data-driven control design. In particular, we consider the quintessential control problem of stabilization by state feedback. 
%
%Recall the definition of informativity for control as given in Definition~\ref{ch1:def:par informativity}. We take the model class $\calM$ and data $\calD$ as before, and take as the control objective $\calO$: `interconnection with a  state feedback controller yields a stable closed loop system'. This means that the set $\Sigma_{\calO}$ of all systems that satisfy the control objective is equal to the set of all stable $n \times n$ matrices
%$$
%M^{n \times n}_{\rm stab}:=  \{ A \in \R^{n \times n} \mid A \text{ is stable} \}.
%$$
%For a given state feedback controller $K\in\mathbb{R}^{m\times n}$, the corresponding set of closed loop systems consistent with the data is equal to 
%\[ 
%\Sigma_{(U_-,X)}(K) = \{ A+ BK \mid (A,B) \in \Sigma_{(U_-,X)} \}. 
%\] 
%In line with Definition \ref{ch1:def:par informativity} we say that the data $(U_-,X)$ are {\em informative for stabilization by state feedback} if there exists a $K \in \mathbb{R}^{m \times n}$ such that $\Sigma_{(U_-,X)}(K) \subseteq M^{n \times n}_{\rm stab}$.
%
%In other words, the input-state data $(U_-,X)$ are informative for stabilization by state feedback if there exists a single real $m \times n$ matrix $K$ such that $A +BK$ is stable for all $(A,B) \in \calM$ that are consistent with the data.
%
%%\begin{remark} 
%At this point, one may wonder about the relation between informativity for stabilizability and informativity for stabilization. It is clear that the data $(U_-,X)$ are informative for stabilizability if $(U_-,X)$ are informative for stabilization by state feedback. However, the reverse statement does not hold in general. This is due to the fact that all systems $(A,B)$ in $\Sigma_{(U_-,X)}$ may be stabilizable, but there may not exist a \emph{common} feedback gain $K$ such that $A+BK$ is stable for all of these systems. 
%%\end{remark}
%
%The following example further illustrates the difference between informativity for stabilizability and informativity for stabilization.
%
%\begin{exm}
%	Consider the scalar system 
%	$
%	\bmx(t+1)= \bmu(t),
%	$
%	where $\bmx, \bmu \in \mathbb{R}$. Suppose that we collect data on the single time interval $\{0,1\}$, specifically, $x(0) = 0$, $u(0) = 1$ and $x(1) = 1$. This means that $U_- = \begin{bmatrix}1 \end{bmatrix}$ and $X = \begin{bmatrix}0 & 1	\end{bmatrix}$. It can be shown that $\Sigma_{(U_-,X)} = \{(a,1) \mid a \in \mathbb{R}\}$. Clearly, all systems in $\Sigma_{(U_-,X)}$ are stabilizable. Nonetheless, the data are not informative for \emph{stabilization}. This is because the systems $(-1,1)$ and $(1,1)$ in $\Sigma_{(U_-,X)}$ cannot be stabilized by the \emph{same} controller of the form $u(t) = K x(t)$. 
%	We conclude that informativity of the data for stabilizability does not imply informativity for stabilization by state feedback. 
%\end{exm}
%
%Having defined the notion of informativity for stabilization, we now take the steps described in the introduction. First, we resolve Problem~\ref{ch1:prob:parametrized}, that is, we find necessary and sufficient conditions for informativity for stabilization by state feedback. After this, we design a corresponding controller, as described in Problem~\ref{ch1:prob:design}. 
%
%In order to do this, we first state a useful lemma. Recall from \eqref{ch2:eq:SigmaD} that $(A,B)\in\Sigma_{(U_-,X)}$ if and only if it is a solution of the the corresponding affine equation. Now let $\Sigma_{(U_-,X)}^0$ denote the solution set of the corresponding homogeneous equation. That is, \begin{equation}\label{ch3:eq:sigma is 0}
%	\Sigma_{(U_-,X)}^0 := \left\{(A_0,B_0) \mid 0 = \bbm A_0 & B_0 \ebm
%	\begin{bmatrix}
%		X_- \\ U_-
%	\end{bmatrix}\right\}.
%\end{equation}
%This allows us to state the following lemma. 
%\begin{lemma}
%	\label{ch3:lemmaF0=0}
%	Suppose that the data $(U_-,X)$ are informative for stabilization by state feedback, and let $K$ be a  feedback gain such that $A+BK$ is stable for all $(A,B) \in \Sigma_{(U_-,X)}$. Then $A_0 + B_0 K = 0$ for all $(A_0,B_0) \in \Sigma_{(U_-,X)}^0$. Equivalently, 
%	$$
%	\im \begin{bmatrix}
%		I \\ K
%	\end{bmatrix} \subseteq 
%	\im \begin{bmatrix}
%		X_- \\ U_-
%	\end{bmatrix}.
%	$$
%\end{lemma}
%
%The solution set of an affine equation is equal to the sum of any solution and the solution set of the corresponding homogenous equation. Since we know that $(A_s,B_s)\in\Sigma_{(U_-,X)}$ by definition, this means that we can write 
%\[ \Sigma_{(U_-,X)} = (A_s,B_s) + \Sigma_{(U_-,X)}^0.\]
%Using this, as a consequence of Lemma~\ref{ch3:lemmaF0=0} we have that if $K$ is a  feedback gain such that $A+BK$ is stable for all $(A,B) \in \Sigma_{(U_-,X)}$, then
%\[ \Sigma_{(U_-,X)}(K) = \{A_s+B_sK\},  \] 
%that is, the set of closed-loop systems consistent with the data is a singleton. It is important to note, however, that this does not mean that $\Sigma_{(U_-,X)}$ is necessarily a singleton.
%
%The above observation turns out to be instrumental in proving the following theorem, which gives necessary and sufficient conditions for informativity for stabilization by state feedback. 
%
%\begin{theorem}
%	\label{ch3:t:algstab}
%	The data $(U_-,X)$ are informative for stabilization by state feedback if and only if the matrix $X_-$ has full row rank and there exists a right inverse $X_-^\sharp$ of $X_-$ such that $X_+ X_-^\sharp$ is stable. 
%	
%	Moreover, $K$ is such that $A +BK$ is stable for all $(A,B) \in \Sigma_{(U_-,X)}$ if and only if $K = U_- X_-^\sharp$, where $X_-^\sharp$ satisfies the above properties. In that case, $A +BK = X_+ X_-^\sharp$ for all $(A,B) \in \Sigma_{(U_-,X)}$.
%\end{theorem}
%
%
%Theorem \ref{ch3:t:algstab} gives a characterization of all input-state data that are informative for stabilization by state feedback and provides a stabilizing controller. Nonetheless, the procedure to compute this controller might not be entirely satisfactory since it is not clear how to find a right inverse of $X_-$ that makes $X_+ X_-^\sharp$ stable. In general, $X_-$ has many right inverses, and $X_+ X_-^\sharp$ can be stable or unstable depending on the particular right inverse $X_-^\sharp$. To deal with this problem and to solve the design problem, we give a characterization of informativity for stabilization in terms of linear matrix inequalities (LMIs). The feasibility of such LMIs can be verified using standard tools. 
%
%\begin{theorem}\label{ch3:t:lmistab}
%	The data $(U_-,X)$ are informative for stabilization by state feedback if and only if there exists a matrix $\Theta \in \mathbb{R}^{T \times n}$ satisfying
%	\begin{equation}
%		\label{ch3:LMI/E}
%		X_- \Theta = (X_- \Theta)^\top\quad\text{ and }\quad
%		\begin{bmatrix}
%			X_- \Theta & X_+ \Theta \\ \Theta^\top X_+^\top & X_- \Theta
%		\end{bmatrix} > 0.
%	\end{equation}
%	Moreover, $K$ is such that $A +BK$ is stable for all $(A,B) \in \Sigma_{(U_-,X)}$ if and only if $K = U_- \Theta (X_-\Theta)^{-1}$ for some matrix $\Theta$ satisfying \eqref{ch3:LMI/E}.
%\end{theorem}
%
%\begin{remark}[Discussion on dd-stab]
%	--- Note work by others, e.g. Claudio. See commented text below. Note the work on sample complexity, where they essentially answer the question: Can we define $X_-,U_-$ with minimal $T$ for which the data are informative? Answer: yes, with $T=n$. https://arxiv.org/pdf/2203.00474.pdf
%%	To the best of our knowledge, LMI conditions for data-driven stabilization were first studied in \cite{DePersis2020}. In fact, the linear matrix inequality \eqref{ch3:LMI/E} is the same as that of \cite[Thm. 3]{DePersis2020}. However, an important difference is that the results in \cite{DePersis2020} assume that the input $ u $ is persistently exciting of sufficiently high order. In contrast, Theorem~\ref{ch3:t:lmistab}, as well as Theorem \ref{ch3:t:algstab}, do not require such conditions. The characterization \eqref{ch3:LMI/E} provides the minimal conditions on the data under which it is possible to obtain a stabilizing controller.
%\end{remark}
%
%The following example provides a simple illustration of the above results. 
%\begin{exm}
%	Consider an unstable system $(A_s,B_s)$, where $A_s$ and $B_s$ are given by
%	$$
%	A_s = \begin{bmatrix}
%		1.5 & 0 \\ 1 & 0.5
%	\end{bmatrix}, \quad B_s = \begin{bmatrix}
%		1 \\ 0
%	\end{bmatrix}.
%	$$
%	We collect data from this system on a single time interval from $t = 0$ until $t = 2$, which results in the data matrices
%	$$
%	X = \begin{bmatrix}
%		1 & 0.5 & -0.25 \\
%		0 & 1 & 1
%	\end{bmatrix}, \quad U_- = \begin{bmatrix}
%		-1 & -1
%	\end{bmatrix}.
%	$$
%	Clearly, the matrix $X_-$ is square and invertible, and it can be verified that
%	$$
%	X_+ X_-^{-1} = \begin{bmatrix}
%		0.5 & -0.5 \\
%		1 & 0.5
%	\end{bmatrix}
%	$$
%	is stable, since its eigenvalues are $\half(1 \pm \sqrt{2}i)$. We conclude by Theorem \ref{ch3:t:algstab} that the data $(U_-,X)$ are informative for stabilization by state feedback. The same conclusion can be drawn from Theorem \ref{ch3:t:lmistab} since $$\Theta = \begin{bmatrix}
%		1 & -1 \\ 0 & 2
%	\end{bmatrix}$$ solves \eqref{ch3:LMI/E}. Next, we can conclude from either Theorem \ref{ch3:t:algstab} or Theorem \ref{ch3:t:lmistab} that the stabilizing feedback gain in this example is unique, and given by $K = U_-X_-^{-1} = \begin{bmatrix}
%		-1 & -0.5
%	\end{bmatrix}$. Finally, it is worth noting that the data are not informative for system identification. In fact, $(A,B) \in \Sigma_{(U_-,X)}$ if and only if
%	$$
%	A = \begin{bmatrix}
%		1.5+a_1 & 0.5a_1 \\ 1+ a_2 & 0.5+ 0.5a_2
%	\end{bmatrix}, \quad B = \begin{bmatrix}
%		1+a_1 \\ a_2
%	\end{bmatrix}
%	$$
%	for some $a_1$ and $a_2 \in \mathbb{R}$.
%\end{exm}
%
\begin{comment}
	\section{The linear quadratic regulator problem}

	An important classical control design problem is the optimal linear quadratic regulator (LQR) problem. In this subsection we will study the data-driven version of this problem within the informativity framework.  

	For given state and input dimensions $n$ and $m$, again consider the model class $\mathcal{M}$ of all discrete-time linear input-state systems \eqref{ch2:e: is-system}.
	%\begin{equation} \label{ch4:e:discintext}
	%\bmx(t+1) = A\bmx(t) + B \bmu(t).
	%\end{equation}
	Consider the discrete time linear system 
	\begin{equation} \label{ch4:e:disc}
	\bmx(t+1) = A \bmx(t) + B \bmu(t),
	\end{equation}
	where $A$  and $B$ are matrices of dimensions $n \times n$ and $n \times m$, and where 
	$\bmx$ is the $n$-dimensional state and $\bmu$ the $m$-dimensional input. In the linear quadratic regulator problem we quantify the performance of the system using a quadratic cost functional $J(x_0,\bmu)$ involving the state trajectory $\bmx$ and the input $\bmu$. The optimal linear quadratic regulator problem is then the problem of finding, for each initial state $x_0$ of the system, an optimal input, i.e. an input that minimizes the cost functional. In this sidebar the basics of discrete-time linear quadratic optimal control are reviewed. In the sequel,  the abbreviation `LQR'  will be used for  `linear quadratic regulator'. 

	For an initial state $x_0$, let $\bmx_{x_0,\bmu}$ be the state sequence of \eqref{ch4:e:disc} resulting from the input $\bmu$ and initial condition $\bmx(0) = x_0$. We omit the subscript and simply write $\bmx$ whenever the dependence on $x_0$ and $\bmu$ is clear from the context. 

	Associated to system \eqref{ch4:e:disc}, we define the quadratic cost functional
	\begin{equation}\label{ch4:e:cost}
	J(x_0,\bmu)=\sum_{t=0}^\infty  \bmx^\top(t) Q \bmx(t) + \bmu^\top(t) R \bmu(t),
	\end{equation}
	where $Q \in \S{n}$ is positive semidefinite and $R \in \S{m}$ is positive definite. Then, the optimal LQR problem is the following: 
	\begin{problem}[The LQR problem]
		Determine for every initial condition $x_0$ an input $\bmu^*$, such that $\lim_{t\to\infty} \bmx_{x_0,\bmu^*}(t) = 0$, and the cost functional $J(x_0,\bmu)$ is minimized under this constraint. 
	\end{problem}
	\noindent Such an input $\bmu^*$ is called optimal for the given $x_0$. Of course, an optimal input does not necessarily exist for all $x_0$. We say that the optimal LQR problem is {\em solvable\/} for $(A,B,Q,R)$ if for every $x_0$ there exists an input ${\bmu}^*$ such that
	\begin{enumerate}
		\item The cost $J(x_0,\bmu^*)$ is finite.
		\item The limit $\lim_{t\to\infty}\bmx_{x_0,\bmu^*}(t)=0$. 
		\item The input $\bmu^*$ minimizes the cost functional, i.e., 
		\[J(x_0,{\bmu}^*)\leq J(x_0,\bar{\bmu})\]
		for all $\bar{\bmu}$ such that $\lim_{t\to\infty}\bmx_{x_0,\bar{\bmu}}(t)=0$.
	\end{enumerate}
	In the sequel, we will require the notion of observable eigenvalues. 
	%Recall from e.g. \cite[Sec. 3.5]{Trentelman2001} that 
	An eigenvalue $\lambda$ of $A$ is called $(Q,A)$-observable if 
	\[ 
	\rank \begin{bmatrix} A-\lambda I \\ Q \end{bmatrix}=n.
	\] 
	The following theorem provides necessary and sufficient conditions for the solvability of the optimal LQR problem for $(A,B,Q,R)$. 
	This theorem is the discrete-time analogue to the continuous-time case stated in \cite[Thm. 10.18]{Trentelman2001}. 
	\begin{theorem}[Conditions for LQR]\label{ch4:t:Harry}
		Let $Q \geq 0$ and $R >0$. Then the following statements hold:	
		\begin{enumerate}
			\item If $(A,B)$ is stabilizable, there exists a unique largest real symmetric solution $P^+$ to the discrete-time algebraic Riccati equation (DARE) 
			\begin{equation}
			\label{ch4:dare}
			P = A^\top PA-A^\top PB(R+B^\top P B)\inv B^\top  P A+Q,
			\end{equation}
			in the sense that $P^+ \geq P$ for every real symmetric $P$ satisfying \eqref{ch4:dare}. The matrix $P^+$ is positive semidefinite.
			\item If, in addition to stabilizability of $(A,B)$, every eigenvalue of $A$ on the unit circle is $(Q,A)$-observable then for every $x_0$ a unique optimal input $\bmu^*$ exists. Furthermore, this input sequence is generated by the feedback law $\bmu = K \bmx$, where
			\begin{equation}
			\label{ch4:optgain}
			K := -(R+B^\top P^+ B)\inv B^\top  P^+ A.
			\end{equation}
			Moreover, the matrix $A+BK$ is stable. 
			\item In fact, the optimal LQR problem is solvable for $(A,B,Q,R)$ if and only if $(A,B)$ is stabilizable and every eigenvalue of $A$ on the unit circle is $(Q,A)$-observable. 
		\end{enumerate}
	\end{theorem}

	If the optimal LQR problem is solvable for $(A,B,Q,R)$, we say that the matrix $K$ given by \eqref{ch4:optgain} is the optimal feedback gain for $(A,B,Q,R)$. 
	% section section_name (end)
	Assume we have input-state data on multiple time intervals, leading to data $\calD:= (U_-,X)$ as given in \eqref{ch2:eq: UXdata}. As before, the set $\Sigma_{\calD}$ of all systems in $\calM$ that are consistent with the data is then given by \eqref{ch2:eq:SigmaD}.
	%equal to 
	%	\begin{equation} 
	%	\label{ch4:eq: SigmaD}
	%	\Sigma_\calD = \left\{ (A,B) \in \calM\mid X_+= \bbm A&B \ebm
	%	\begin{bmatrix}
	%	X_-\\U_-
	%	\end{bmatrix} \right\}.
	%	\end{equation}
	We assume that the data are generated by the true (but unknown) system $(A_s,B_s)$, which is therefore in $\Sigma_\calD$ itself.

	In the context of the optimal LQR problem the control objective $\calO$ is: `the system must be  controlled using the optimal feedback gain'.  In order to formalize this, we introduce the following notation.
	For any given $K$, let $\Sigma^{Q,R}_{K}$ denote the set of all systems of the form \eqref{ch2:e: is-system} for which $K$ is the optimal feedback gain corresponding to $Q$ and $R$, that is,
	\[ 
	\Sigma_K^{Q,R}:=\set{(A,B) \in \calM }{K \text{ is optimal for }(A,B,Q,R)}.
	\]
	This gives rise to yet another notion of informativity in line with Definition~\ref{ch1:def:par informativity}. Indeed, informativity requires the existence of a single feedback gain that is optimal for all systems consistent with the data. For the definition of solvability of the optimal LQR problem we refer to the sidebar `The linear quadratic regulator problem`.
	\begin{definition}[Informativity for LQR]
		Given matrices $Q$ and $R$, we say that the data $\calD = (U_-,X)$ are \emph{informative for optimal linear quadratic regulation} if the optimal LQR problem is solvable for all $(A,B) \in \Sigma_{\calD} $ and there exists $K$ such that $\Sigma_{\calD} \subseteq \Sigma^{Q,R}_{K}$.
	\end{definition}
	An instrumental result in obtaining necessary and sufficient conditions for informativity for optimal linear quadratic regulation is the following lemma.
	\begin{lemma}[Common solution of the Riccati equation]\label{ch4:l: same P works for all}
	Let $Q=Q^\top$ be positive semidefinite and $R=R^\top$ be positive definite. Suppose the data $(U_-,X)$ are informative for optimal linear quadratic regulation. Let $K$ be such that $\Sigma_{\calD} \subseteq \Sigma^{Q,R}_{K}$. Then, there exist a square matrix $M$ and a positive semidefinite matrix $P^+$ such that for all $(A,B)\in\Sigma_{\calD}$
	\begin{align}
	&\!\!\!\!\!M=A+BK,\label{ch4:e: define M}\\
	&\!\!\!\!\!P^+\!= A^\top\! P^+\!A\! -\! A^\top\! P^+\!B(R + B^\top\! P^+\! B)\inv B^\top\!  P^+\! A + Q,\!\!\label{ch4:e:dare aux}\\
	&\!\!\!\!\!P^+-M^\top P^+M=K^\top RK+Q,\label{ch4:e:lyap aux}\\
	&\!\!\!\!\!K=-(R+B^\top P^+ B)\inv B^\top  P^+ A.\label{ch4:e:uni K aux}
	\end{align}
	\end{lemma}
	Statement \eqref{ch4:e:dare aux} of the lemma says that if the data are informative, there exists a common solution $P^+ \geq 0$ to the whole collection of AREs associated with systems $(A,B)$ that are consistent with the data. Statement \eqref{ch4:e:uni K aux} 
	says that if $K$ is the common optimal gain for all systems that are consistent with the data, then it must be of the expected form \eqref{ch4:e:uni K aux} for all  $(A,B)$ consistent with the data. According to \eqref{ch4:e: define M}, the optimal closed loop system matrices $A + BK$ are identical for all consistent pairs $(A,B)$.

	The following theorem gives necessary and sufficient conditions for informativity for optimal linear quadratic regulation. 
	\begin{theorem}[Conditions for informativity \cite{8960476}]\label{ch4:t:LQinform}
		Let $Q \geq 0$ and $R > 0$. Then the data $(U_-,X)$ are informative for optimal linear quadratic regulation if and only if at least one of the following two conditions hold:
		\begin{enumerate}
			\item\label{ch4:cond:a} The data $(U_-,X)$ are informative for identification, that is, $\Sigma_{\calD}=\pset{(A_s,B_s)}$, and the optimal LQR problem is solvable for $(A_s,B_s,Q,R)$. In this case, the optimal feedback gain $K$ is of the form \eqref{ch4:e:uni K aux} where $P^+$ is the largest real symmetric solution to \eqref{ch4:e:dare aux} with $A = A_s$ and $B = B_s$.
			\item\label{ch4:cond:b} For all $(A,B) \in \Sigma_{\calD}$ we have $A=A_s$. Moreover, $A_s$ is stable, $QA_s = 0$, and the optimal feedback gain is given by $K = 0$. 
		\end{enumerate}
	\end{theorem}

	This theorem should be interpreted as follows. Condition \ref{ch4:cond:b}) of Theorem \ref{ch4:t:LQinform} can be considered as a  pathological case in which the only $A$ consistent with the data is the true one, namely $A_s$. This matrix $A_s$ is stable and $QA_s = 0$. Since $\bmx(t) \in \im A_s$ for all $t > 0$, we have $Q \bmx(t) = 0$ for all $t > 0$ if the input function is chosen as $\bmu = 0$. Additionally, since $A_s$ is stable, this shows that the optimal input is equal to $\bmu^* = 0$. If we set aside the pathological case \ref{ch4:cond:b}), the main message of Theorem \ref{ch4:t:LQinform} is the following: if the data are informative for optimal linear quadratic regulation they are also informative for system identification, in the sense that the set of systems consistent with the data contains only one element, i.e., $\Sigma_{\calD} = \{(A_s,B_s) \}$. This observation is consistent with the paper \cite{Polderman1986} that showed the necessity of identifiability of the true system in adaptive LQ control.

	At first sight, this might seem like a negative result in the sense that data-driven LQR is only possible with data that are also informative enough to uniquely identify the system. However, at the same time, Theorem \ref{ch4:t:LQinform} can be viewed as a positive result in the sense that it provides fundamental justification for the data conditions imposed in e.g. \cite{DePersis2020}. Indeed, in \cite{DePersis2020} the data-driven infinite horizon LQR problem\footnote{Note that the authors of \cite{DePersis2020} formulate this problem as the minimization of the $H_2$-norm of a certain transfer matrix.} is solved using input-state data under the assumption that the input is persistently exciting of sufficiently high order. Under the latter assumption, the input-state data are informative for system identification, i.e., the matrices $A_s$ and $B_s$ can be uniquely determined from data. Theorem \ref{ch4:t:LQinform} justifies such a strong assumption on the richness of data in data-driven linear quadratic regulation.
	%
	%
	The data-driven \emph{finite} horizon LQR problem was solved under a persistency of excitation assumption in \cite{Markovsky2007}. Our results suggest that also in this case informativity for system identification is necessary for data-driven LQR, although further analysis is required to prove this claim.
	%\end{remark}

	Although Theorem \ref{ch4:t:LQinform} gives necessary and sufficient conditions under which the data are informative for optimal linear quadratic regulation, it might not be directly clear how these conditions can be verified given the input-state data. Therefore, in what follows we rephrase the conditions of Theorem \ref{ch4:t:LQinform} in terms of the data matrices $X$ and $U_-$.


	\begin{theorem}[Alternative conditions for informativity \cite{8960476}]
		\label{ch4:t:LQinform2}
		Let $Q \geq 0$ and $R >0$. Then the data $(U_-,X)$ are informative for optimal linear quadratic regulation if and only if at least one of the following two conditions hold:
		\begin{enumerate}
			\item\label{ch4:cond:a2} The data $(U_-,X)$ are informative for identification, equivalently, there exists $\begin{bmatrix} V_1 & V_2\end{bmatrix}$ such that 
	\begin{equation}\label{ch2:eq:V1 V2} \begin{bmatrix} X_- \\ U_- \end{bmatrix} \begin{bmatrix} 	V_1 & V_2	\end{bmatrix} = \begin{bmatrix} I_n & 0 \\ 0& I_m\end{bmatrix}. \end{equation}		
	Moreover, the optimal LQR problem is solvable for $(A_s,B_s,Q,R)$, where $A_s=X_+V_1$ and $B_s=X_+V_2$.
			\item\label{ch4:cond:b2} There exists $\Theta \in \mathbb{R}^{T \times n}$ such that $X_- \Theta = (X_- \Theta)^\top$, 	$U_- \Theta = 0 $, 
			\begin{equation}
			\label{ch4:e:LMI/E/K/Q}
			\begin{bmatrix}
			X_- \Theta & X_+ \Theta \\ \Theta^\top X_+^\top & X_- \Theta
			\end{bmatrix} > 0.
			\end{equation}
			and $ QX_+\Theta = 0$. 
		\end{enumerate}
	\end{theorem}

	It is also possible to directly compute the optimal LQR feedback gain $K$ from the given data. 
	%For this, we will employ ideas from the study of Riccati inequalities (see e.g \cite{Ran1988}).
	%The main idea is to replace the Riccati inequality by the linear matrix inequality $\mathcal{L}(P) \leq 0$, where
	%\begin{equation}
	%\label{ch3:dataineq}
	%\mathcal{L}(P) := \xmt P\xm-\xpt P\xp -\xmt Q\xm-\umt R\um.
	%\end{equation}
	%Note that the linear operator $\mathcal{L}$ is completely defined by the data matrices $X$ and $U_-$, and the weight matrices $Q$ and $R$. 
	%
	Indeed, the following theorem asserts that $P^+$ as in Lemma~\ref{ch4:l: same P works for all} can be found as the unique solution to an optimization problem involving only the data. Furthermore, the optimal feedback gain $K$ can subsequently be found by solving a set of linear equations. In the sequel, for a given square matrix $M$, $\trace(M)$ will denote the trace of $M$.

	\begin{theorem}[A semi-definite programming approach \cite{8960476}]
		\label{ch4:t:LQgaindata}
	Let $Q \geq 0$ and $R > 0$. Suppose that the data $(U_-,X)$ are informative for optimal linear quadratic regulation. Consider the linear operator $P\mapsto\calL(P)$ defined by
	$$
	\mathcal{L}(P) := \xmt P\xm-\xpt P\xp -\xmt Q\xm-\umt R\um.
	$$
	Let $P^+$ be as in Lemma~\ref{ch4:l: same P works for all}. The following statements hold:
		\begin{enumerate}
			\item \label{ch4:semidefiniteprogram} The matrix $P^+$ is equal to the unique solution to the optimization problem
			\begin{align*}
			\text{ maximize } \: &\trace(P) \\
			\text{ subject to } \: &P  \geq 0 
			\,\,\text{ and }\,\,  \mathcal{L}(P) \leq 0.
			\end{align*} 
			\item There exists a right inverse $X_-^\sharp$ of $X_-$ such that
				\begin{align}
				\label{ch4:eq:1}
				\mathcal{L}(P^+) X_-^\sharp &= 0.
				\end{align}
			Moreover, if $X_-^\sharp$ satisfies \eqref{ch4:eq:1}, then the optimal feedback gain is given by $K = U_- X_-^\sharp$.
		\end{enumerate}
	\end{theorem}
	From a design viewpoint, the optimal feedback gain $K$ can be found in the following way. First solve the semidefinite program in Theorem \ref{ch4:t:LQgaindata}. Subsequently, compute a solution $X_-^\sharp$ to the linear equations $X_- X_-^\sharp = I$ and \eqref{ch4:eq:1}. Then, the optimal feedback gain is given by $K = U_- X_-^\sharp$.

\end{comment}
\section{Dissipativity analysis}
\subsection{Dissipativity}
Now, we will analyse the informativity problems for dissipativity of FDLTI systems using both exact and noisy data.
Consider the discrete system state-space system \ref{ch5:e:lin-sys} discussed before.
% In this subsection, we study dissipativity of linear finite-dimen\-sional input-state-output systems from a data-driven perspective. 
% With the definitions and analysis in the previous section, we can directly state an equivalent formulation for dissipativity from noise-free input and state trajectories. The necessary and sufficient condition is a simple LMI that can be solved using standard solvers. 
% We generalize our definition of dissipativity given in \ref{eq:dds} to incorporate variety of supply functions $S$. Consider a discrete-time linear input-state-output system as mentioned in 
% \begin{align}
% \bmx(t+1)&=A \bmx(t)+B \bmu(t), \\
% \bmy(t)&=C \bmx(t)+D \bmu(t),
% \end{align}
% where $A\in\R^{n\times n}$, $B\in\R^{n\times m}$, $C\in\R^{p\times n}$, and $D\in\R^{p\times m}$ are given matrices. 
\begin{theorem}[\cite{vanwaarde2023informativity}, Dissipativity LMI]
\label{thm:diss_lmi}
The system \eqref{ch5:e:lin-sys} is said to be \emph{dissipative\/} with respect to the {\em supply rate}  
\beq\label{ch5:e:supply}
s(u,y)=\bbm u\\y\ebm^\top S \bbm u\\y\ebm\quad\text{where $S \in \S{m+p}$}
\eeq
if there exists $P \in \S{n}$ with $P \geq 0$ such that the following holds 
\beq\label{ch5:eq:KY_- P}
\bbm
I & 0 \\A & B
\ebm^\top
\bbm
P & 0\\0 & -P
\ebm
\bbm
I & 0 \\A & B
\ebm+
\bbm
0 & I\\C & D
\ebm^\top
S
\bbm
0 & I\\C & D
\ebm
\geq 0.
\eeq
Or equivalently,
\begin{equation}
    \label{ch5:eq:eqM1}
    \sysone^\top\!\! \underbrace{\bbm
    P & 0 & 0 & 0\\
    0 & F & 0 & G\\
    0 & 0 & -P & 0\\
    0 & G^\top & 0 & H
    \ebm}_{M} \sysone \geq 0
    \end{equation} 
where we partition $S$ as follows
\begin{equation} \label{ch5:eq:partitionS}
    S=\bbm F & G\\G^\top & H\ebm \qquad\text{where  $F\in\R^{m\times m}$, $G\in\R^{m\times p}$, $H\in\R^{p\times p}$}
\end{equation}
\end{theorem}

% %
% with  $\hat{Q} = C^\top QC$, $\hat{S} = C^\top S + C^\top QD$ and $\hat{R} = D^\top QD + (D^\top S + S^\top D) + R$.
% These learnings can be formalised into the following theorem
% \begin{theorem}
% Suppose that the system~\eqref{ch5:e:lin-sys} is controllable and let $s$ be a quadratic supply rate of the form~\eqref{ch5:e:supply}. Then the following statements are equivalent.
% \begin{itemize}
% \item[a)] The system is $(Q,S,R)$-dissipative.
% \item[b)] There exists a quadratic storage function $V(x) \coloneqq x^\top P x$ with $P = P^\top \succeq 0$ 
% such that 
% \begin{align*}
% V(x_{k+1}) - V(x_k) \leq s(u_k,y_k)
% \end{align*}
% for all $k$ and all $(u,x,y)$ satisfying \eqref{ch5:e:lin-sys}.
% \item[c)] There exists a matrix $P = P^\top \succeq 0$ such that 
% \begin{align}
% \label{eq:diss_lmi}
% \begin{bmatrix} A^\top PA - P - \hat{Q} & A^\top PB - \hat{S} \\
% (A^\top PB - \hat{S} )^\top & -\hat{R} + B^\top P B \end{bmatrix} \preceq 0
% \end{align}
% %
% with  $\hat{Q} = C^\top QC$, $\hat{S} = C^\top S + C^\top QD$ and $\hat{R} = D^\top QD + (D^\top S + S^\top D) + R$.
% \end{itemize}

% \end{theorem}
\subsection{Dissipativity from input-state-output trajectories}
To solve the informativity problem now, we use the notion of persistently exciting input data. We will first consider the exact case and then the noisy case. As the system matrices are unknown, we use a subscript $s$ to denote the same. 
% Using the notion of persistently exciting input data and then we employ the informativity approach to derive necessary and sufficient conditions.

% In the framework of data-driven system analysis,  The question we want to study then is whether we can verify dissipativity using only the input-state-output data obtained from the unknown system. In the present section we will study this question for the situation that our data are noiseless.
\subsubsection{Exact case}
Consider the unknown input-state-output system
\bse\label{ch5:e:tru-sys}
\begin{align}
\bmx(t+1)&=A_{s} \bmx(t)+B_{s} \bmu(t),\\
\bmy(t)&=C_{s} \bmx(t)+D_{s} \bmu(t),  \end{align}
\ese
with $\bmu(t) \in \mathbb{R}^m$, $\bmx(t) \in \mathbb{R}^n$ and $\bmy(t) \in\R^{p}$ the input, state and output 
with the assumption that the dimensions $m,n$ and $p$ are known. The combined data is given by $\calD = (U_-,X,Y_-)$ which satisfies
\begin{equation}  \label{ch5:e:true system compatible}
%\label{ch3:dataeq}
\begin{bmatrix}
X_+ \\ Y_-
\end{bmatrix} = \begin{bmatrix}
A_s & B_s \\ C_s & D_s
\end{bmatrix} \begin{bmatrix}
X_- \\ U_-
\end{bmatrix}.
\end{equation}
The set of all systems that are consistent with these data is then given by: 
\begin{equation}
\label{ch5:eq:sigma iso}
\Sigma_{(U_-,X,Y_-)}: = \left\{ (A,B,C,D) \mid \begin{bmatrix} X_+ \\ Y_- \end{bmatrix} = \begin{bmatrix} A & B \\ C & D \end{bmatrix} \begin{bmatrix} X_- \\ U_- \end{bmatrix} \right\}.
\end{equation}
And hence, $(A_s,B_s,C_s,D_s) \in \Sigma_{(U_-,X,Y_-)}$.
% We 
% Our goal is to infer from the data $(U_- ,X,Y_- )$ whether the unknown system \eqref{ch5:e:tru-sys} is dissipative.  
% These data are assumed to be generated by the true system $(A_s,B_s, C_s,D_s)$, which means that
% \begin{theorem}[\cite{9551767}]
% \label{thm:Hill}
% The system~\eqref{ch5:e:tru-sys} is dissipative w.r.t. the supply rate $s$ in \eqref{ch5:e:supply} if and only if
% %
% \begin{align}
% \sum_{k=0}^r s(u_k,y_k)
% \geq0, \quad \forall r \geq 0,
% \label{eq:diss}
% \end{align}
% %
% for all trajectories $\{u_k,y_k\}_{k=0}^\infty$ of \eqref{ch5:e:tru-sys} with initial condition $x_0=0$. 
% \end{theorem}
% The 
% \subsubsection{Noiseless data}
% We now define the  property of \emph{informativity for } for the case of noiseless data.
\begin{definition}[\cite{vanwaarde2023informativity}, Informativity for dissipativity of noiseless data]\label{def:dd diss}
The data $(U_-,X,Y_-)$ is \emph{informative for dissipativity\/} with respect to the supply rate \eqref{ch5:e:supply} if there exists a matrix $P \in \S{n}$, $P \geq0$, such that the LMI \eqref{ch5:eq:KY_- P} holds for every system $(A,B,C,D) \in \Sigma_{(U_-,X,Y_-)}$. 
\end{definition}
% Note that our definition of informativity for dissipativity requires the systems in $\Sigma_{(U_-,X,Y_-)}$ to be dissipative with a \emph{common} storage function. 
This definition requires all the systems in $\Sigma_{(U_-,X,Y_-)}$ to be dissipative with a \emph{common} storage function which introduces additional assumptions on $S$ such that the its inertia\footnote{number of negative, zero, and positive eigenvalues of $S$ respectively, given by the tuple $(\rho_-, \rho_0, \rho_+)$} is $\In(S)=(p,0,m)$.
% We will restrict ourselves to the case that the number of negative eigenvalues of the matrix $S$ representing the supply rate is equal to the output dimension $p$ and the number of positive eigenvalues of $S$ is equal to the input dimension $m$. In particular then, $S$ is nonsingular. In other words, we will impose the following assumption on the inertia of $S$ which is given by the tuple $(\rho_-, \rho_0, \rho_+)$. These are the .
% \begin{equation} \label{ch5:e:inertia}
% \In(S)=(p,0,m).
% \end{equation}
% It is a well-known fact that a necessary condition for dissipativity of any system of the form \eqref{ch5:e:lin-sys} is that the input dimension does not exceed the positive signature of $S$. Our assumption requires that the input dimension is equal to this positive signature and in addition that the matrix $S$ is nonsingular. This assumption is satisfied, for example, for the positive-real and bounded-real case.  Indeed, in the positive-real case we have that $m = p$ and
% $$
% S = \begin{bmatrix}
% 0 & I_m \\ I_m & 0
% \end{bmatrix},
% $$
% so that $\In(S) = (m,0,m)$. In the bounded-real case we have 
% $$
% S = \begin{bmatrix}
% \gamma^2 I_m & 0 \\ 0 & -I_p
% \end{bmatrix}
% $$
% for some $\gamma > 0$, which implies that $\In(S)=(p,0,m)$. 

% Before establishing conditions for informativity for dissipativity, we note that $\Sigma_{(U_-,X,Y_-)}$ contains exactly one element if and only if
% \beq\label{ch5:e:full row rank}
% \rank \bbm X_-\\U_- \ebm = n + m.
% \eeq
% in this case, we say the data  $(U_-,X,Y_-)$ are \emph{informative for system identification}.

% As the main result of this part we will now show that the noiseless input-state-output data $(U_-,X,Y_-)$ are informative for dissipativity if and only if they are informative for system identification and the unique system consistent with these data is dissipative. In addition, dissipativity of this unknown true system can be expressed in terms of feasibility of an LMI involving the data.
\begin{theorem}[\cite{vanwaarde2023informativity}, Informativity for dissipativity of noiseless data] \label{ch5:th:info diss}
Assuming that $\In(S)=(p,0,m)$ and the data $(U_- ,X,Y_- )$ is informative for system identification, it is informative for dissipativity with respect to the supply rate \eqref{ch5:e:supply} if and only if there exists $P=P^\top \geq0$ such that
\beq\label{ch5:e:exact cond2}
\bbm
X_-\\X_+
\ebm^\top
\bbm
P & 0\\0 & -P
\ebm
\bbm
X_-\\X_+
\ebm+
\bbm
U_- \\Y_- 
\ebm^\top
S
\bbm
U_- \\Y_- 
\ebm
\geq 0.
\eeq
\end{theorem}
\subsubsection{Noisy Data}
Now, we add the unknown process noise and measurement noise to the system \eqref{ch5:e:tru-sys} resulting in the below system
% Next, we proceed with studying informativity for dissipativity in the case that our input-state-output data are obtained from an unknown system subject to unknown process noise and measurement noise. We assume that the unknown system is given by 
\bse \label{ch5:e:tru-sys with noise}
\begin{align}
\bmx(t+1)&=A_{s} \bmx(t)+B_{s} \bmu(t)  + \bmw(t),\\
\bmy(t)&=C_{s} \bmx(t)+D_{s} \bmu(t) + \bmz(t),  \end{align}
\ese
where $\bmu(t) \in \mathbb{R}^m$, $\bmx(t) \in \mathbb{R}^n$ and $\bmy(t) \in\R^{p}$ are the input, state and output with the assumption that the dimensions $m,n$ and $p$ are known. The unknown noise terms are $\bmw(t) \in \mathbb{R}^n$ and $\bmz(t) \in \mathbb{R}^p$ representing process and measurement noise, respectively. Again, we assume the assumptions on supply rate such that $S \in \S{m + p}, \In(S)=(p,0,m)$.

As this is the noisy case, we will need to make assumptions about noise to proceed any further.
% Also the system matrices $(A_{s}, B_{s},C_{s},D_{s})$ are assumed to be unknown.
% Again, we assume that a supply rate is represented by a given matrix $S \in \S{m + p}$, viz. \eqref{ch5:e:supply}.
%The problem that we will study is whether we can determine whether the unknown system \eqref{ch5:e:tru-sys with noise} is dissipative with respect to the given supply rate. 
\begin{assumption}[\cite{vanwaarde2023informativity}, Noise model] \label{ch5:assumption on noise samples}
The noise samples satisfy the quadratic matrix inequality
\begin{equation} 
    \label{ch5:asnoise}
    \begin{bmatrix}
    I \\ V_-^\top 
    \end{bmatrix}^\top 
    \Phi
    \begin{bmatrix}
    I \\ V_-^\top 
    \end{bmatrix} \geq 0
\end{equation}
% $w(0),w(1),\dots,w(T-1)$ and $z(0),z(1),\dots,z(T-1)$, 
% collected in the real $(n + p) \times T$ matrix 
where
\begin{equation} \label{ch5:eq:Phi}
\underbrace{V_-}_{\in\mathbb{R}^{(n+p)\times T}} := \bbm w(0) & w(1) & \cdots & w(T-1) \\  z(0) & z(1)  & \cdots & z(T-1)    \ebm \quad\text{and}\quad\underbrace{\Phi}_{\in \bpi_{n +p, T}} = \bbm \overbrace{\Phi_{11}}^{\in \S{n + p}}  & \overbrace{\Phi_{12}}^{\in \mathbb{R}^{(n + p) \times T}} \\ \underbrace{\Phi_{21}}_{\Phi_{12}^\top} & \underbrace{\Phi_{22}}_{\in \S{T}} \ebm
\end{equation}
where $\bpi_{m,n}$ denotes following block partitioned matrices $E\in\bpi_{m,n}=\bbm \overbrace{E_{11}}^{\in \S{m}}  & \overbrace{E_{12}}^{\in \mathbb{R}^{m \times n}} \\ \underbrace{E_{21}}_{E_{12}^\top} & \underbrace{E_{22}}_{\in \S{n}} \ebm$

Also, let
\begin{equation} \label{ch0:e:Zr}
\calZ_{T}(\Phi):=\left\{ Z\in\R^{T\times {(n+p)}} \mid \bbm I_{n+p}\\Z\ebm^\top\Phi\bbm I_{n+p}\\Z\ebm\geq 0\right\},
\end{equation}
As $\Phi \in \bpi_{n + p,T}$, $\calZ_T(\Phi)$ is non-empty and convex.\\
Now,  $V_-$ satisfies \eqref{ch5:asnoise} if and only if $V_-^\top \in \calZ_T(\Phi)$.
%(see Sidebar ``Quadratic matrix inequalities").
\end{assumption}
% Suppose that we obtain  input-state-output data data from the unknown system  \eqref{ch5:e:tru-sys with noise}. These data are collected in the matrices $(U_-,X,Y_-)$. The auxiliary matrices $X_-$ and $X_+$ are as defined before. The noise terms $\bmw$ and $\bmz$ are unknown, so $w(0),w(1),\dots,w(T-1)$ and  $z(0),z(1),\dots,z(T-1)$ are not measured, and are therefore not part of the data. 
% We do have the following information on the noise during the data sampling period.

% We now turn to defining the  property of \emph{informativity for dissipativity} for noisy input-state-output data, i.e. data that are generated by the unknown system \eqref{ch5:e:tru-sys with noise} with unknown process noise and measurement noise whose samples satisfy the quadratic matrix inequality \eqref{ch5:asnoise}. 
% As our model class $\calM$  we take all noisy input-state-output systems
% \bse \label{ch5:e:model class}
% \begin{align}
% \bmx(t+1)&=A \bmx(t)+B \bmu(t)  + \bmw(t),\\
% \bmy(t)&=C \bmx(t)+D \bmu(t) + \bmz(t),  \end{align}
% \ese
% with input dimension $m$, state space dimension $n$ and output dimension $p$. Given the input-state-output data $(U_-,X,Y_-)$ together with the information that the matrices of noise samples satisfy \eqref{ch5:asnoise}, the set of all systems consistent with the data is then given by 
Similar to our exact case, we define $\Sigma_\calD$ as follows
\begin{equation} \label{ch5:def:SigmaD}
\Sigma_{\calD} = \left \lbrace (A,B,C,D) \! \mid \! \left(\begin{bmatrix} X_+\\Y_-  \end{bmatrix} \!-\! \begin{bmatrix} A&B\\
C&D\end{bmatrix}\!\begin{bmatrix}X_-\\ U_-  \end{bmatrix}\right)^\top \!\in\!\calZ_T(\Phi)  \right \rbrace.
\end{equation}
% We assume that the data have been obtained from the unknown system \eqref{ch5:e:tru-sys with noise}, i.e., $(A_s,B_s,C_s,D_s) \in \Sigma_{\calD}$. Therefore, $\Sigma_{\calD}$ is nonempty.
We also define another matrix $N$
\begin{equation} \label{ch5:eq:bigN}
N\!:= \!\begin{pmat}[{|}]
N_{11} & N_{12} \cr\- N_{12}^\top & N_{22} \cr
\end{pmat} \! = \! \left[\begin{array}{c|c}
I & \begin{array}{c}
X_+\\Y_- 
\end{array}
\\\hline
0 & \begin{array}{c}
-X_-\\-U_- 
\end{array}
\end{array}\right]
\!\!
\bbm
\Phi_{11} & \Phi_{12}\\
\Phi_{21} & \Phi_{22}
\ebm\!\!
\left[\begin{array}{c|c}
I & \begin{array}{c}
X_+\\Y_- 
\end{array}
\\\hline
0 & \begin{array}{c}
-X_-\\-U_- 
\end{array}
\end{array}\right]^\top\!\!
\end{equation}
such that $(A,B,C,D)\in \Sigma_{\calD}$ if and only if
\beq \label{ch5:e:char N2 model}
\bbm
I\\\hline\\[-3mm]
\begin{matrix}
A^\top & C^\top\!\\
B^\top & D^\top\!
\end{matrix}
\ebm^\top\!\! 
N
\bbm
I\\\hline\\[-3mm]
\begin{matrix}
A^\top & C^\top\!\\
B^\top & D^\top\!
\end{matrix}
\ebm
\geq 0.\equiv\qquad
    \bbm
    A^\top & C^\top\!\\
    B^\top & D^\top\!
    \ebm \in \calZ_{n + m}(N).
\eeq
% This can be restated equivalently as
% $$
% \bbm
% A^\top & C^\top\!\\
% B^\top & D^\top\!
% \ebm \in \calZ_{n + m}(N).
% $$
We can now do a similar analysis to solve the problem.
\begin{definition}[\cite{vanwaarde2023informativity}, Informativity for dissipativity of noisy data]\label{ch5:def:info diss noisy}
The noisy input-state-output data $(U_-,X,Y_-)$ are \emph{informative for dissipativity\/} with respect to the supply rate \eqref{ch5:e:supply} if there exists a matrix $P\geq 0$ such that the LMI \eqref{ch5:eq:KY_- P} holds for all systems $(A,B,C,D)\in\Sigma_{\calD}$. 
\end{definition}
% Note that, we will require that all systems consistent with the data are dissipative with a {\em common storage function}.
% Similar to the noiseless case as studied before, in the remainder of this section we will assume that the matrix $S$ representing the supply rate satisfies the inertia condition $\In(S)=(p,0,m)$.

% The following preliminary lemma states that also in the context of noisy data, the rank condition \eqref{ch5:e:full row rank} on the input-state data is necessary for informativity.

% \begin{lemma}[\cite{vanwaarde2023informativity}, Necessity of full row rank condition] \label{ch5:lem:necc noisy case} 
% Assume that $\In(S)=(p,0,m)$. If the data $(U_- ,X,Y_- )$ are informative for dissipativity with respect to the supply rate \eqref{ch5:e:supply} then \eqref{ch5:e:full row rank} holds.
% \end{lemma}

% In addition, we need the following lemma which states that if the data are informative for dissipativity with all systems in $\Sigma_{\calD}$ having a given common storage function $P \geq 0$, then $P$ is necessarily \emph{positive definite}. This is true under the additional assumption that . Combining this with the fact that $N \in \bpi_{n+p, n+m}$ as was already established above, this implies that the set $\Sigma_{\calD}$ has a nonempty interior.
% Now, to resolve the problem of system identification using this data, we will 
\begin{lemma}[\cite{9781292}, Necessity of positive definite storage]
\label{ch5:lem:P>0}
In addition to the assumption on the inertia of $S$, we also need the Schur complement $N \schur N_{22} $ to be positive definite, then the common storage function $P$ is necessarily positive	definite.
\end{lemma}

% Our next step is to partition 
% \begin{equation} \label{ch5:eq:partitionS}
% S=\bbm F & G\\G^\top & H\ebm,
% \end{equation}
% where  $F\in\R^{m\times m}$, $G\in\R^{m\times p}$, $H\in\R^{p\times p}$. For any $P \geq 0$ define
% \begin{equation} \label{ch5:eq:partitionM}
% M:=\bbm
% P & 0 & 0 & 0\\
% 0 & F & 0 & G\\
% 0 & 0 & -P & 0\\
% 0 & G^\top & 0 & H
% \ebm.
% \end{equation}
% Then the system $(A,B,C,D)$ can be seen to satisfy the dissipation inequality \eqref{ch5:eq:KY_- P} if and only if  
% \begin{equation}
% \label{ch5:eq:eqM1}
% \sysone^\top\!\! M \sysone \geq 0
% \end{equation} 
% Moreover, with this notation in place, the problem of characterizing informativity for dissipativity is equivalent to finding conditions  for the existence of a matrix $P > 0$ such that the inequality \eqref{ch5:eq:eqM1}
% %\begin{equation}
% %\label{ch5:eq:eqM1}
% %\sysone^\top\!\! M \sysone \geq 0
% %\end{equation} 
% holds for all $(A,B,C,D)$ satisfying the inequality \eqref{ch5:e:char N2 model}.
% %\begin{equation}
% %\label{ch5:eq:eqN1}
% %\systwo^\top\!\! N \systwo \geq 0 \; .
% %\end{equation}
We invoke the non-strict matrix S-lemma to solve such problem,
% Our strategy to solve this problem is to invoke the nonstrict matrix S-lemma.
\begin{theorem}[\cite{9781292}, Matrix S-lemma]
\label{t:nonstrictS-lemma}
For $M,N \in \mathbb{S}^{q+r}$, if there exists a real $\alpha \geq 0$ such that $M - \alpha N \geq 0$ then $\calZ_r(N) \subseteq \calZ_r(M)$.
Also, assuming $N \in \bpi_{q,r}$ and $N$ has at least one positive eigenvalue, then $\calZ_r(N) \subseteq \calZ_r(M)$ if and only if there exists a real $\alpha \geq 0$ such that $M-\alpha N \geq 0$. 
\end{theorem}
% Similar to the `standard' S-lemma we note that the matrix S-lemma requires $N$ to have at least one 
% 
% positive eigenvalue, an assumption known as the \emph{Slater condition}.
% 
To apply this lemma, an additional dualization result is needed as terms of \eqref{ch5:eq:eqM1} and \eqref{ch5:e:char N2 model} are \emph{transpose} of each other. 
\begin{lemma}[\cite{9781292}, Dualization of dissipation inequality]\label{ch5:lem:diss dual}
For  $P > 0$, define
\begin{equation}
\label{ch5:eq:Shat}
 \hat{S}:=\begin{bmatrix} 0&-I_p\\
 I_m&0 \end{bmatrix} S\inv \begin{bmatrix} 0&-I_m\\I_p&0 \end{bmatrix}.
\end{equation}
Then we have
\begin{equation}
\label{ch5:eq:eqL1}
\begin{bmatrix}
I&0\\
A&B
\end{bmatrix}^\top\!\!\begin{bmatrix}
P&0\\
0&-P
\end{bmatrix}\begin{bmatrix}
I&0\\
A&B
\end{bmatrix}+\begin{bmatrix}
0&I\\
C&D
\end{bmatrix}^\top\!\! S \begin{bmatrix}
0&I\\
C&D
\end{bmatrix}\geq 0
\end{equation}
if and only if 
\begin{align}\label{ch5:eq:L2}
\begin{bmatrix}
I&0 \!\\
A^\top&C^\top\!
\end{bmatrix}^\top\!\!\!\begin{bmatrix}
P^{-1}&0\\
0&-P^{-1}
\end{bmatrix}\!\!\!
\begin{bmatrix}
I&0\!\! \\
A^\top&C^\top\!\!
\end{bmatrix}\!\!+\!\!\begin{bmatrix}
0&I \!\! \\
B^\top&D^\top \!\!
\end{bmatrix}^\top\!\!\!\! \hat{S}\!\! \begin{bmatrix}
0&I \!\! \\
B^\top&D^\top \!\!
\end{bmatrix} \!\!\geq\! 0.
\end{align}
\end{lemma}
Intuitively, it connects the storage functions of the dual systems using an inverse relationship.
% Lemma~\ref{ch5:lem:diss dual} can be interpreted as saying that the system defined by the quadruple $(A,B,C,D)$ is dissipative with respect to the supply rate $S$, with storage function $P$ if and only if the dual system $(A^\top,C^\top,B^\top,D^\top)$ is dissipative with respect to the supply rate $\hat{S}$, with storage function $P^{-1}$.% A behavioral analogue of this result was obtained in \cite{Willems2002}, Proposition 12. 

Now, we will do similar partitioning procedure for this dual system,
$$
-S^{-1} = \begin{bmatrix}
\hatF&\hatG\\
\hatG^\top  &\hatH
\end{bmatrix}\qquad\text{where $\hatF=\hatF^\top \in\S{m}$, $\hatG\in\R^{m\times p}$, and $\hatH=\hatH^\top \in\S{p}$ }
$$
and define 
\begin{equation} \label{ch5:eq:Mhat}
\hat{M} := \begin{bmatrix}
    P\inv & 0 & 0 & 0 \\
    0 & \hatH & 0 &-\hatG^\top\\
    0 & 0 & -P\inv & 0 \\
    0 & -\hatG & 0 & \hatF
    \end{bmatrix}.
\end{equation}
Then it is easily seen that $(A^\top, C^\top,B^\top, D^\top)$ satisfies the inequality \eqref{ch5:eq:L2} if and only if 
\begin{equation}
\label{ch5:eq:M1}
\bbm
I\\\hline\\[-3mm]
\begin{matrix}
A^\top & C^\top\!\\
B^\top & D^\top\!
\end{matrix}
\ebm^\top\!\! \hat{M} \bbm
I\\\hline\\[-3mm]
\begin{matrix}
A^\top & C^\top\!\\
B^\top & D^\top\!
\end{matrix}
\ebm \geq 0.
\end{equation} 
% We may now observe that, under the assumptions that $\In(S)=(p,0,m)$ and schure complement of block $N_{22}$ is $>0$, informativity for dissipativity with respect to the supply rate given by $S$ holds if and only if there exists $P>0$ such that the quadratic inequality \eqref{ch5:eq:M1} holds for all $(A,B,C,D)$ that satisfy the the quadratic inequality \eqref{ch5:e:char N2 model}, equivalently 
% \begin{equation} \label{ch5:eq:Z-inclusion}
% \calZ_{n +m}(N) \subseteq \calZ_{n+m}(\hat{M}).
% \end{equation}
% This brings us in position to apply Theorem \ref{t:nonstrictS-lemma} and to obtain the following characterization for informativity for dissipativity for noisy input-state-output data.
\bthe[\cite{vanwaarde2023informativity}, Informativity for dissipativity of noisy data] \label{ch5:t:noise 1}
The data $(U_-,X,Y_-)$ with noise as stated in \ref{ch5:assumption on noise samples}. Under the assumption of, $\In(S) = (p,0,m)$ and $N \schur N_{22} >0$,
% Partition 
% \begin{equation} \label{ch5:eq:partition of Sinv}
% -S\inv = \begin{bmatrix}
% \hatF&\hatG\\
% \hatG^\top  &\hatH
% \end{bmatrix},
% \end{equation}
% where $\hatF=\hatF^\top \in\R^{m\times m}$, $\hatG\in\R^{m\times p}$, and $\hatH=\hatH^\top \in\R^{p\times p}$. 
the data is informative for dissipativity with respect to the supply rate \eqref{ch5:e:supply} if and only if there exist a real matrix $Q \in \S{n}$, $Q >0$ and a scalar $\alpha\geq 0$ such that 
\begin{equation}
\begin{bmatrix}
    \! Q & \!0\! & \!0\! & 0 \!\!\! \\
    \! 0 & \!\hatH\! & \!0\! &-\hatG^\top \!\!\! \\
   \! 0 & \!0\! & \!-Q\! & 0 \!\!\! \\
   \! 0 & \!-\hatG\! & \!0\! & \hatF \!\!\!
    \end{bmatrix} \!-\! \alpha\!\!
    \None
     \!\!\!\!\geq\! 0. \label{ch5:eq:LMI2}
\end{equation}
The common storage function is given by $P : = Q^{-1}$.
\ethe
This case requires the knowledge of state data which is expensive and might not be readily available. Hence, the next section tackles the informativity for dissipativity problem using only input-output trajectories.
% Theorem~\ref{ch5:t:noise 1} provides a tractable method for verifying informativity for dissipativity of noisy data given the noise model introduced in Assumption~\ref{ch5:assumption on noise samples}. The procedure involves solving the linear matrix inequality \eqref{ch5:eq:LMI2} for $Q$ and $\alpha$. Given $Q$, a common storage function $P$ for all systems in $\Sigma_{\calD}$ is also readily computable as $P = Q^{-1}$. 
\subsection{Dissipativity from input-output trajectories}
In this case, we don't have state data, the combined data is given by $\calD = (U_-,Y_-)$. This absence adds another complexity to the problem and hence we introduce another term for our next assumption.
\begin{definition}[\cite{9551767}]
    The lag $\underline{l}$ of system~\eqref{ch5:e:tru-sys} is the smallest integer $l \in \mathbb{N}_{+}$ such that the observability matrix $\mathcal{O}_l$ has rank $n$
    \begin{align}
    \mathcal{O}_l \coloneqq \begin{bmatrix} C \\ CA \\ \vdots \\ CA^{l-1}
    \end{bmatrix}
    \end{align}
    \label{def:lag}
\end{definition}
\subsubsection{Noiseless data}
\label{sec:output}
%
Now, we extend the original system \eqref{ch5:e:tru-sys} into another system where the state is given by only the known inputs and outputs under the assumption that the upper bound on the lag of the system is known as $l \geq \underline{l}$
\begin{lemma}[\cite{9551767}]
\label{lem:extended}
There exists a system $\widetilde{G}$ with system matrices $\widetilde{A},\widetilde{B},\widetilde{C},\widetilde{D}$ with the same input-output behavior as the original system, i.e., there exists $\xi_0$ such that for $k=0,\dots,N-1$,
\begin{align}\label{eq:sys_stacked}
\xi_{k+1} = \widetilde{A} \xi_k + \widetilde{B} u_k, \quad
y_k = \widetilde{C} \xi_k + \widetilde{D} u_k,
\end{align}
where the extended state is defined by
\begin{align*}
{
\xi_{k} = \begin{bmatrix} u_{k-l}^\top & u_{k-l+1}^\top & \cdots & u_{k-1}^\top & y_{k-l}^\top & y_{k-l+1}^\top & \cdots & y_{k-1}^\top \end{bmatrix}^\top.}
\end{align*} 
\end{lemma}
\begin{proof}
The state equations can be rewritten as \eqref{eq:input-output},
\begin{align}
\begin{bmatrix} y_{k-l} \\ y_{k-l+1} \\ \vdots \\ y_{k-1} \end{bmatrix} = \underbrace{\begin{bmatrix} C_s \\ C_sA_s \\ \vdots \\ C_sA_s^{l-1} \end{bmatrix}}_{\mathcal{O}_l} x_{k-l} + 
\underbrace{\begin{bmatrix} 
D_s & 0 & & \dots & 0 & 0 \\
C_sB_s & D _s& & \dots & 0 & 0 \\
\vdots & \ddots & \ddots&\ddots&\ddots&\vdots \\
C_sA_s^{l-2}B_s & C_sA_s^{l-3}B_s &\dots & C_sA_sB_s & C_sB_s &D_s
\end{bmatrix} }_R
\begin{bmatrix} u_{k-l} \\ u_{k-l+1} \\ \vdots \\ u_{k-1} \end{bmatrix}
\label{eq:input-output}
\end{align}
which can be simplified to
%
\begin{align*}
\begin{bmatrix} 
-R & I
\end{bmatrix} \xi_k = \mathcal{O}_l x_k.
\end{align*}
%Note that with
Now, as the lag $\underline{l}\leq l$, $\mathcal{O}_l$ has full column rank, and hence, there exists a left-inverse $\mathcal{O}_l^{-1}$ (which has full row rank) such that
\begin{align*}
\underbrace{\mathcal{O}_l^{-1} \begin{bmatrix} 
-R & I
\end{bmatrix}}_{K} \xi_k = x_k.
% \label{eq:trafo}
\end{align*}
This gives us
% The general system description from \eqref{ch5:e:tru-sys} yields
\begin{align*}
y_{k} = C_s A_s^l x_{k-l} + \begin{bmatrix} C_sA_s^{l-1} B_s & \dots & C_sB_s \end{bmatrix} \begin{bmatrix} u_{k-l} \\ \vdots \\ u_{k-1} \end{bmatrix}.
\end{align*}
which can be expanded as,
% Together with $K$, %$T=\mathcal{O}_l^{-1}\begin{bmatrix}-R I\end{bmatrix}$
% this leads to,
\begin{align}
\begin{split}
\label{eq:extended_sys}
\underbrace{\begin{bmatrix} u_{k-l+1} \\ \vdots \\ u_{k-1} \\ u_{k} \\ y_{k-l+1} \\ \vdots \\ y_{k-1} \\ y_{k}  \end{bmatrix}}_{\xi_{k+1}} &= \underbrace{\begin{bmatrix} 
0 & I & \dots & 0 & 0 &  0& \dots &0 \\
\vdots & \ddots & \ddots&\ddots& \vdots & & \ddots&\vdots \\
0 & 0 & \dots & I & 0& 0&  \dots & 0 \\
0 & 0 & \dots & 0 & 0 & 0&  \dots & 0 \\
0 & 0 & \dots & 0 & 0& I& \dots &0 \\
\vdots & \ddots & \ddots&\vdots&\vdots & & \ddots&\vdots \\
0 & 0 & \dots & 0 & 0& 0&  \dots & I \\
C_sA_s^{l-1}B_s & \dots & \dots & C_sB_s & 0 & 0 & \dots &  0 \\
\end{bmatrix} + \begin{bmatrix} 0 \\ \vdots \\ 0\\ 0 \\ 0 \\ \vdots \\ 0 \\ C_s A_s^l T \end{bmatrix} }_{\widetilde{A}} \underbrace{\begin{bmatrix} u_{k-l} \\ u_{k-l+1} \\ \vdots \\ u_{k-1} \\ y_{k-l} \\ y_{k-l+1} \\ \vdots \\ y_{k-1} \end{bmatrix}}_{\xi_{k}} + \underbrace{\begin{bmatrix} 0 \\ \vdots \\ 0 \\ I \\ 0 \\ \vdots \\ 0 \\ D_s  \end{bmatrix}}_{\widetilde{B}} u_{k} \\
y_k &= \underbrace{\begin{bmatrix} 0 & \dots & 0 & I \end{bmatrix} \widetilde{A}}_{\widetilde{C}} \xi_k + \underbrace{D}_{\widetilde{D}} u_k
\end{split} 
\end{align}
% This proves that $(\widetilde{A}, \widetilde{B}, \widetilde{C}, \widetilde{D})$ can explain the input-output trajectory. 
\end{proof} 
% While Lem.~\ref{lem:extended} is a well-known fact, we nevertheless add a proof in the appendix for completeness and to provide some intuition.

% The converse of Lem.~\ref{lem:extended} follows trivially from its proof and the constructed extended system in \eqref{eq:extended_sys}: All input-output trajectories of the extended system~\eqref{eq:sys_stacked} with zero initial condition $\xi_0 = 0$ (or $\xi_0 \in \mathcal{X}_\xi$, where $\mathcal{X}_\xi$ denotes the set of reachable states) are also input-output trajectories of the system~\eqref{ch5:e:tru-sys}.
Now, we can get the informativity results by a analysis similar to the input-state-output case \ref{ch5:th:info diss}.
The data matrices will have to be replaced appropriately with the new state $\bold{\xi}$ as follows
\begin{align*}
    \Xi_- &\coloneqq \begin{bmatrix} \xi_l & \xi_{l+1} & \cdots & \xi_{T-1} \end{bmatrix}, \\
    \Xi_+ &\coloneqq \begin{bmatrix} \xi_{l+1} & \xi_{l+2} & \cdots & \xi_{T} \end{bmatrix}, \\ 
    Y_{\Xi_-} &\coloneqq \begin{bmatrix} y_{l} & y_{l+1} & \cdots & y_{T-1} \end{bmatrix}, \\
    U_{\Xi_-} &\coloneqq \begin{bmatrix} u_{l} & u_{l+1} & \cdots & u_{T-1} \end{bmatrix},
\end{align*}
\begin{theorem}[Informativity for dissipativity of noiseless data] \label{ch5:th:infodissio}
    Assuming that $\In(S)=(p,0,m)$, the lag of the system $\underline{l}\leq l$ and the data $(U_- ,Y_- )$ is informative for system identification, then the data $(U_-,Y_- )$ is informative for dissipativity with respect to the supply rate \eqref{ch5:e:supply} if and only if there exists $P=P^\top \geq0$ such that
    \beq\label{ch5:e:exact cond3}
    \bbm
    \Xi_-\\\Xi_+
    \ebm^\top
    \bbm
    P & 0\\0 & -P
    \ebm
    \bbm
    \Xi_-\\\Xi_+
    \ebm+
    \bbm
    U_{\Xi_-} \\Y_{\Xi_-} 
    \ebm^\top
    S
    \bbm
    U_{\Xi_-} \\Y_{\Xi_-} 
    \ebm
    \geq 0.
    \eeq
    
    \end{theorem}
\begin{comment}
We can hence conclude that 
the extended system~\eqref{eq:sys_stacked} has the same input-output behavior as \eqref{ch5:e:tru-sys} if the initial condition $\xi_0$ is restricted to the set of reachable states.  
This implies that both systems have the same input-output behavior for zero initial condition $\xi_0 = 0, x_0 = 0$. 
Together with Thm.~\ref{thm:Hill}, this in turn implies that dissipativity of the extended system~\eqref{eq:sys_stacked} (if the initial condition is reachable) 
is equivalent to dissipativity of \eqref{ch5:e:tru-sys}. 
Hence, using Lem.~\ref{lem:extended}, we can reduce the problem of verifying dissipativity from input-output trajectories to the problem of verifying dissipativity from input-state trajectories of the potentially non-minimal system~\eqref{eq:sys_stacked}.

\begin{remark}
In a purely data-driven setup, knowledge on the lag $\underline{l}$ is often not available. However, as shown above, it is sufficient for the purpose of this section to have an upper bound on $l$ or even an upper bound an $n$ since $\underline{l} \leq n$. 
\end{remark}
Similar to the results of this section,~\cite{Persis2020} uses such an extended state to design data-driven controllers but, instead of the lag $\underline{l}$, the system order $n$ is used. For MIMO systems, this can result in significantly larger state dimensions. Furthermore, \cite{Persis2020} assumes controllability of the extended system, which is generally not the case, unless $l=n$ and SISO systems are considered.

Before stating our main result of this section, we recall the Fundamental Lemma introduced in \cite{willems2005note}: 
\begin{lemma}[Fundamental Lemma]
\label{lem:fundamental}
Suppose $\{u_k,y_k\}_{k=0}^{N-1}$ is a trajectory of a controllable LTI system $G$, where $u$ is persistently exciting of order $l+n$.
Then, $\{\bar{u}_k,\bar{y}_k\}_{k=0}^{l-1}$ is a trajectory of $G$ if and only if there exists $\alpha\in\mathbb{R}^{N-l+1}$ such that
%
\begin{align*}
\begin{bmatrix}H_l(u)\\H_l(y)\end{bmatrix}\alpha
=\begin{bmatrix}\bar{u}\\\bar{y}\end{bmatrix}.
\end{align*}
\end{lemma}

Note that the Fundamental Lemma requires controllability. However, since system~\eqref{ch5:e:tru-sys} that generated the data is assumed to be minimal, we can apply Lem.~\ref{lem:fundamental} to describe all input-output trajectories of~\eqref{ch5:e:tru-sys} and hence also of the extended system for $\xi_0 \in \mathcal{X}_\xi$ applying Lem.~\ref{lem:extended}.
Using Lem.~\ref{lem:extended} and Lem.~\ref{lem:fundamental}, we can hence determine dissipativity properties from input-output trajectories.
For this, 
we collect the extended state data analogously to Sec.~\ref{sec:diss} in the following form
\begin{align*}
\Xi &\coloneqq \begin{bmatrix} \xi_l & \xi_{l+1} & \cdots & \xi_{N-1} \end{bmatrix}, \\
\Xi_+ &\coloneqq \begin{bmatrix} \xi_{l+1} & \xi_{l+2} & \cdots & \xi_{N} \end{bmatrix}, \\ 
Y_\Xi &\coloneqq \begin{bmatrix} y_{l} & y_{l+1} & \cdots & y_{N-1} \end{bmatrix}, \\
U_\Xi &\coloneqq \begin{bmatrix} u_{l} & u_{l+1} & \cdots & u_{N-1} \end{bmatrix},
\end{align*}
%%
which directly leads us to the main result of this section.

\begin{theorem}
\label{thm:3}
Given an input-output trajectory $\{u_k, y_k\}_{k=0}^{N-1}$ of a controllable LTI system $G$ of the form~\eqref{ch5:e:tru-sys} with lag $\underline{l}$. Let $l \geq \underline{l}$ and consider the feasibility problem to find $P=P^\top\succeq0$ such that
\begin{align}
\begin{split}
&\Xi_+^\top P \Xi_+ - \Xi^\top P \Xi \\
&- Y_\Xi^\top Q Y_\Xi  
- Y_\Xi^\top SU_\Xi - (SU_\Xi)^\top Y_\Xi - U_\Xi^\top R U_\Xi \preceq 0.
\end{split}
\label{eq:opt_output}
\end{align}
\begin{enumerate}
\item If there exists a $P=P^\top\succeq0$ such that \eqref{eq:opt_output} holds  and additionally $\{u_k\}_{k=0}^{N-1}$ is persistently exciting of order $n+l+1$, then $G$ is $(Q,S,R)$-dissipative.
\item If there exists no $P=P^\top\succeq0$ such that \eqref{eq:opt_output} holds, then $G$ is not $(Q,S,R)$-dissipative.
\end{enumerate} 
\end{theorem}
%
\begin{proof} 
1) First, we notice that the 
data matrix $\Xi$ can be written as 
\begin{align*}
\Xi = \begin{bmatrix} H_l (\{u_k\}_{k=0}^{N-2}) \\ H_l (\{y_k\}_{k=0}^{N-2}) \end{bmatrix} = \begin{bmatrix}
u_0 & u_{1} & \cdots & u_{N-l-1} \\
u_1 & u_{2} & \cdots & u_{N-l} \\ 
\vdots & & & \vdots \\
u_{l-1} & u_{l} & \cdots & u_{N-2} \\
y_{0} & y_{1} & \cdots & y_{N-l-1} \\
y_{1} & y_{2} & \cdots & y_{N-l} \\
\vdots & & & \vdots \\
y_{l-1} & y_{l} & \cdots & y_{N-2} \\
\end{bmatrix}.
\end{align*}
Since there exists a controllable realization (of order $n$) with the same input-output behavior as the extended system, 
the Fundamental Lemma implies that the image of 
$\Xi$ spans the whole reachable state space of the extended system $\mathcal{X}_\xi$. More specifically, if $\{u_k\}_{k=0}^{N-1}$ is persistently exciting of order $n+l$, then Lem.~\ref{lem:fundamental} guarantees that the columns in $\Xi$ span all possible input-output trajectories of the system $G$, and hence the whole reachable state space of the extended system~\eqref{eq:sys_stacked}.
If $\{u_k\}_{k=0}^{N-1}$ is persistently exciting of order $n+l+1$, 
then it additionally holds that $\begin{bmatrix}\Xi\\U_\Xi\end{bmatrix}$ spans the space of all input-state trajectories of~\eqref{eq:sys_stacked}.

Using $\Xi_+=\widetilde{A}\Xi+\widetilde{B}U_\Xi$ and rearranging~\eqref{eq:opt_output}, we obtain
\begin{align}
\begin{bmatrix} \Xi \\ U_\Xi \end{bmatrix}^\top 
\begin{bmatrix} \widetilde{A}^\top P \widetilde{A} - P - \hat{Q} & \widetilde{A}^\top P \widetilde{B} - \hat{S} \\
(\widetilde{A}^\top P \widetilde{B} - \hat{S} )^\top & -\hat{R} + \widetilde{B}^\top P \widetilde{B} \end{bmatrix} 
 \begin{bmatrix} \Xi \\ U_\Xi \end{bmatrix} \preceq 0,
\label{eq:proof11}
\end{align}
with $\hat{Q}$, $\hat{S}$, $\hat{R}$ similar as in \eqref{eq:diss_lmi}.
Since $\begin{bmatrix}\Xi\\U_\Xi\end{bmatrix}$ spans the space of all input-state trajectories, this implies
\begin{align}
\begin{bmatrix} \xi_k \\ u_k \end{bmatrix}^\top 
\begin{bmatrix} \widetilde{A}^\top P \widetilde{A} - P - \hat{Q} & \widetilde{A}^\top P \widetilde{B} - \hat{S} \\
(\widetilde{A}^\top P \widetilde{B} - \hat{S} )^\top & -\hat{R} + \widetilde{B}^\top P \widetilde{B} \end{bmatrix} 
 \begin{bmatrix} \xi_k \\ u_k \end{bmatrix} \leq 0
\label{eq:proof1}
\end{align}
for all $k$ and all trajectories $(u,\xi)$ of the extended system~\eqref{eq:sys_stacked} (with $\xi_0 \in \mathcal{X}_\xi$). 
Hence, there exists a quadratically lower bounded storage function for the extended system~\eqref{eq:sys_stacked} satisfying the dissipation inequality. This implies that the system~\eqref{eq:sys_stacked} is $(Q,S,R)$-dissipative which in turn, using Thm.~\ref{thm:Hill} and Lem.~\ref{lem:extended}, implies that the system~\eqref{ch5:e:tru-sys} is $(Q,S,R)$-dissipative.

2) We prove this direction via contraposition. If system~\eqref{ch5:e:tru-sys} is $(Q,S,R)$-dissipative, then, according to Thm.~\ref{thm:diss_lmi}, there exists a quadratic storage function $V(x_k) = x_k^\top P^\prime x_k$ such that
\begin{align*}
x_{k+1}^\top P^\prime x_{k+1} - x_k^\top P^\prime x_k \leq s(u_k, y_k)
\end{align*}
holds for all $k$ and all $(u,x,y)$ satisfying~\eqref{ch5:e:tru-sys}. From the proof of Lem.~\ref{lem:extended}, we know that there exists a transformation matrix $T$ such that $x_{k} = T \xi_k$ holds for all reachable states $\xi_k$ and all $k$. Hence, the matrix $P = T^\top P^\prime T\succeq0$ 
satisfies \eqref{eq:proof1} for all $k$ and all $(u,\xi)$ of the extended system~\eqref{eq:sys_stacked}. 
Using the Fundamental Lemma~\cite{willems2005note}, this implies that \eqref{eq:proof11} holds and thus there exists a $P\succeq0$ such that \eqref{eq:opt_output} holds.
\end{proof}

Thm.~\ref{thm:3} provides an equivalent formulation of dissipativity based on input-output data.
The result itself and its proof are conceptually similar to the state measurements case in Thm.~\ref{thm:1}.
A key challenge is that, in contrast to the matrix $\begin{bmatrix}X\\U\end{bmatrix}$, the matrix $\begin{bmatrix}\Xi\\U_\Xi\end{bmatrix}$ does usually not have full row rank, even if the input is persistently exciting, since the system~\eqref{eq:sys_stacked} is usually not controllable.
However, the Fundamental Lemma implies that, assuming the input to be persistently exciting of order $l+n$, the matrix $\Xi$ spans the space of all state trajectories of the extended system~\eqref{eq:sys_stacked}.
Under the stronger assumption of persistence of excitation of order $l+n+1$, which we assume in Thm.~\ref{thm:3}, it even holds that $\begin{bmatrix}\Xi\\U_\Xi\end{bmatrix}$ spans the space of all input-state trajectories of~\eqref{eq:sys_stacked}.
Using this fact, it is then straightforward to derive~\eqref{eq:opt_output}, which provides an equivalent data-driven characterization of dissipativity.

\begin{remark}
In Thm.~\ref{thm:3}, a necessary and sufficient condition for $(Q,S,R)$-dissipativity is given, which can be extended to cyclo-dissipativity for $P \nsucceq 0$. However, for $l > \underline{l}$ it is generally difficult to verify strict dissipativity (with $P \succ 0$ and a strict definiteness condition in \eqref{eq:opt_output}), since $\begin{bmatrix}\Xi\\U_\Xi\end{bmatrix}$ does usually not have full row rank in these cases. Finding conditions for strict dissipativity from input-output data with $l >\underline{l}$ is therefore an interesting issue for future research.
\end{remark}
\end{comment}
\subsubsection{Noisy data}

\label{sec:output_noise}
%
Now, consider the case of noisy input-output data. For this, we can rewrite the system~\eqref{ch5:e:tru-sys} in the difference operator form
\begin{align}
\begin{split}
 y_k = &-a_{l} y_{k-1} - \dots - a_2 y_{k-l+1} - a_1 y_{k-l} + d u_{k} + b_{l} u_{k-1} + \dots + b_2 u_{k-l+1} + b_1 u_{k-l},
\end{split}
\label{eq:sys_diff}
\end{align}
with $a_i \in \mathbb{R}^{p \times p}$, $b_i \in \mathbb{R}^{p \times m}$, $i=1, \dots, l$, and $l$ is an upper bound on the lag $\underline{l} \leq l$.

Now, the input-output behavior is corrupted by process noise of the following form
\begin{align}
\begin{split}
 y_k = &-a_{l} y_{k-1} - \dots - a_2 y_{k-l+1} - a_1 y_{k-l} + d u_{k} + b_{l} u_{k-1} + \dots + b_2 u_{k-l+1} + b_1 u_{k-l} + b_v v_k,
\end{split}
\label{eq:sys_diff_with_noise}
\end{align}
where $v_k \in \mathbb{R}^{m_v}$ denotes the noise and $b_v \in \mathbb{R}^{p \times m_v}$ is an extra term which can be used to utilise prior knowledge of noise. 

Now, this behavior ~\eqref{eq:sys_diff_with_noise} can also be represented in state-space as shown below
%
\begin{align}
\begin{split}
\label{eq:extended_sys_bw}
\begin{bmatrix} u_{k-l+1} \\ \vdots \\ u_{k-1} \\ u_{k} \\ y_{k-l+1} \\ \vdots \\ y_{k-1} \\ y_{k} \end{bmatrix} = \begin{bmatrix} 
0 & I & \dots & 0 & 0 & 0 & \dots & 0  \\
\vdots & \ddots & \ddots&\ddots& \vdots & & \ddots&\vdots \\
0 & 0 & \dots & I & 0& 0&  \dots & 0 \\
0 & 0 & \dots & 0 & 0 & 0&  \dots & 0 \\
0 & 0 & \dots & 0 & 0& I& \dots &0 \\
\vdots & \ddots & \ddots&\vdots&\vdots & & \ddots&\vdots \\
0 & 0 & \dots & 0 & 0& 0&  \dots & I \\
b_1 & b_2 & \dots & b_l & -a_1 & -a_2 & \dots &  -a_l 
\end{bmatrix}  
\begin{bmatrix} u_{k-l} \\ u_{k-l+1} \\ \vdots \\ u_{k-1} \\ y_{k-l} \\ y_{k-l+1} \\ \vdots \\ y_{k-1}  \end{bmatrix} + 
\begin{bmatrix} 
0 \\ \vdots \\ 0 \\ I \\ 0 \\ \vdots \\ 0 \\ D  \end{bmatrix} 
u_{k} + \begin{bmatrix} 0 \\ \vdots \\ 0 \\ 0 \\ 0 \\ \vdots \\ 0 \\  b_v \end{bmatrix} v_{k} 
\end{split} 
\end{align}
%
This can be simplified as
\begin{align}
\begin{split}
\label{eq:extended_sys2}
\xi_{k+1} &= 
\underbrace{\begin{bmatrix} 
\widetilde{A}_1 \\
\widetilde{C} \end{bmatrix} }_{\widetilde{A}}
\xi_k + \underbrace{\begin{bmatrix} \widetilde{B}_1 \\ \widetilde{D} \end{bmatrix}}_{\widetilde{B}} u_{k} + \begin{bmatrix} 0 \\  b_v \end{bmatrix} v_{k}, \\
y_k &= \widetilde{C} \xi_k + \widetilde{D} u_k + b_v v_{k},
\end{split} 
\end{align}
where $\widetilde{A}\in\mathbb{R}^{(p+m)l \times (p+m)l}$, $\widetilde{B}\in\mathbb{R}^{(p+m)l\times m}$, $\widetilde{C} \in \mathbb{R}^{p \times (p+m)l}$, and $\widetilde{D} \in \mathbb{R}^{p \times m}$, 

Here, $\widetilde{A}_1 \in \mathbb{R}^{((p+m)l-p) \times (p+m)l}$, and $\widetilde{B}_1 \in \mathbb{R}^{((p+m)l-p) \times m}$ are fixed.

Now, to incorporate noise, we can consider the following noise model
\begin{assumption}[Noise model] \label{ch5:assumption on noise samplesio}
The noise samples satisfy the quadratic matrix inequality
\begin{equation} 
    \label{ch5:asnoise}
    \begin{bmatrix}
    I \\ V_-^\top 
    \end{bmatrix}^\top 
    \Phi
    \begin{bmatrix}
    I \\ V_-^\top 
    \end{bmatrix} \geq 0
\end{equation}
% $w(0),w(1),\dots,w(T-1)$ and $z(0),z(1),\dots,z(T-1)$, 
% collected in the real $(n + p) \times T$ matrix 
where
\begin{equation} \label{ch5:eq:Phiio}
\underbrace{V_-}_{\in\mathbb{R}^{m_v\times T-l}} := \bbm v(l) & v(l+1) & \cdots & v(T-1)    \ebm \quad\text{and}\quad\underbrace{\Phi}_{\in \bpi_{m_v, T-l}} = \bbm \overbrace{\Phi_{11}}^{\in \S{m_v}}  & \overbrace{\Phi_{12}}^{\in \mathbb{R}^{m_v \times (T-l)}} \\ \underbrace{\Phi_{21}}_{\Phi_{12}^\top} & \underbrace{\Phi_{22}}_{\in \S{T-l}} \ebm
\end{equation}
Also, let
\begin{equation} \label{ch0:e:Zr}
\calZ_{T-l}(\Phi):=\left\{ Z\in\R^{(T-l)\times {m_v}} \mid \bbm I_{m_v}\\Z\ebm^\top\Phi\bbm I_{m_v}\\Z\ebm\geq 0\right\},
\end{equation}
As $\Phi \in \bpi_{m_v,T-l}$, $\calZ_{T-l}(\Phi)$ is non-empty and convex.\\
Now,  $V_-$ satisfies \eqref{ch5:asnoise} if and only if $V_-^\top \in \calZ_{T-l}(\Phi)$.
%(see Sidebar ``Quadratic matrix inequalities").
\end{assumption}
% Suppose that we obtain  input-state-output data data from the unknown system  \eqref{ch5:e:tru-sys with noise}. These data are collected in the matrices $(U_-,X,Y_-)$. The auxiliary matrices $X_-$ and $X_+$ are as defined before. The noise terms $\bmw$ and $\bmz$ are unknown, so $w(0),w(1),\dots,w(T-1)$ and  $z(0),z(1),\dots,z(T-1)$ are not measured, and are therefore not part of the data. 
% We do have the following information on the noise during the data sampling period.

% We now turn to defining the  property of \emph{informativity for dissipativity} for noisy input-state-output data, i.e. data that are generated by the unknown system \eqref{ch5:e:tru-sys with noise} with unknown process noise and measurement noise whose samples satisfy the quadratic matrix inequality \eqref{ch5:asnoise}. 
% As our model class $\calM$  we take all noisy input-state-output systems
% \bse \label{ch5:e:model class}
% \begin{align}
% \bmx(t+1)&=A \bmx(t)+B \bmu(t)  + \bmw(t),\\
% \bmy(t)&=C \bmx(t)+D \bmu(t) + \bmz(t),  \end{align}
% \ese
% with input dimension $m$, state space dimension $n$ and output dimension $p$. Given the input-state-output data $(U_-,X,Y_-)$ together with the information that the matrices of noise samples satisfy \eqref{ch5:asnoise}, the set of all systems consistent with the data is then given by 
Similar to our previous case, we define $\Sigma_\calD$ as follows after noting $\widetilde{A}_1, \widetilde{B}_1$ are known, hence we can reduce the size of our noise model 
\begin{multline}\label{ch5:def:SigmaD}
\Sigma_{\calD} =  \Biggl\{ (A,B,C,D) \! \mid \!\\
A = \bbm \widetilde{A}_1\\C\ebm, B = \bbm \widetilde{B}_1\\D\ebm, \left(\begin{bmatrix} Y_{\Xi_-}  \end{bmatrix} \!-\! \begin{bmatrix}
C&D\end{bmatrix}\!\begin{bmatrix}\Xi_-\\ U_{\Xi_-}  \end{bmatrix}\right)^\top = b_v V \text{ and } V\!\in\!\calZ_{T-l}(\Phi)\Biggr\}
\end{multline}

% We assume that the data have been obtained from the unknown system \eqref{ch5:e:tru-sys with noise}, i.e., $(A_s,B_s,C_s,D_s) \in \Sigma_{\calD}$. Therefore, $\Sigma_{\calD}$ is nonempty.
Now, the only thing that needs to be taken care of is the matrix $S$, which needs to be extended suitably to get the equivalent $\widetilde{S}\in\S{(m+p)l + pl}$ for the extended system. Then partition $\widetilde{S}$ as shown below, and construct a matrix $M$ similar to the input-state-output case \eqref{ch5:eq:KY_- P} and then use the Matrix S-lemma \ref{t:nonstrictS-lemma} to get the final result.
\begin{equation} \label{ch5:eq:partition of Sinv}
-\widetilde{S}^{-1} = \begin{bmatrix}
\hatF&\hatG\\
\hatG^\top  &\hatH
\end{bmatrix}\qquad\text{where $\hatF=\hatF^\top \in\S{(m+p)l}$, $\hatG\in\R^{(m+p)l\times pl}$, and $\hatH=\hatH^\top \in\S{pl}$ }
\end{equation}
\bthe[\cite{vanwaarde2023informativity}, Informativity for dissipativity of noisy data] \label{ch5:t:noise 1}
The data $(U_-,Y_-)$ with noise as stated in \ref{eq:sys_diff}. Under the assumption of, $\In(S) = (p,0,m)$ and $N \schur N_{22} >0$,
% Partition 
% \begin{equation} \label{ch5:eq:partition of Sinv}
% -S\inv = \begin{bmatrix}
% \hatF&\hatG\\
% \hatG^\top  &\hatH
% \end{bmatrix},
% \end{equation}
% where $\hatF=\hatF^\top \in\R^{m\times m}$, $\hatG\in\R^{m\times p}$, and $\hatH=\hatH^\top \in\R^{p\times p}$. 
the data is informative for dissipativity with respect to the supply rate \eqref{ch5:e:supply} if and only if there exist a real matrix $Q \in \S{(m+p)l}$, $Q >0$ and a scalar $\alpha\geq 0$ such that 
\begin{equation}
    \begin{bmatrix}
        \! Q & \!0\! & \!0\! & 0 \!\!\! \\
        \! 0 & \!\hatH\! & \!0\! &-\hatG^\top \!\!\! \\
       \! 0 & \!0\! & \!-Q\! & 0 \!\!\! \\
       \! 0 & \!-\hatG\! & \!0\! & \hatF \!\!\!
        \end{bmatrix} \!-\! \alpha
        \left[\begin{array}{c|c}
            I & \begin{array}{c}
            \Xi_+\\Y_{\Xi_-} 
            \end{array}
            \\\hline
            0 & \begin{array}{c}
            -\Xi_-\\-U_{\Xi_-} 
            \end{array}
            \end{array}\right]
        \begin{bmatrix}
        \Phi_{11}  & \Phi_{12} \\ \Phi_{21} & \Phi_{22}
        \end{bmatrix}
        \left[\begin{array}{c|c}
            I & \begin{array}{c}
            \Xi_+\\Y_{\Xi_-} 
            \end{array}
            \\\hline
            0 & \begin{array}{c}
            -\Xi_-\\-U_{\Xi_-} 
            \end{array}
            \end{array}\right]^\top
        \geq\! 0. \label{ch5:eq:LMI3}
    \end{equation}
The common storage function is given by $P : = Q^{-1}$.
\ethe
\begin{comment}
We also define another matrix $N$
\begin{equation} \label{ch5:eq:bigN}
N\!:= \!\begin{pmat}[{|}]
N_{11} & N_{12} \cr\- N_{12}^\top & N_{22} \cr
\end{pmat} \! = \! \left[\begin{array}{c|c}
I & \begin{array}{c}
\Xi_+\\Y_{\Xi_-} 
\end{array}
\\\hline
0 & \begin{array}{c}
-\Xi_-\\-U_{\Xi_-}
\end{array}
\end{array}\right]
\!\!
\bbm
\Phi_{11} & \Phi_{12}\\
\Phi_{21} & \Phi_{22}
\ebm\!\!
\left[\begin{array}{c|c}
I & \begin{array}{c}
\Xi_+\\Y_{\Xi_-} 
\end{array}
\\\hline
0 & \begin{array}{c}
-\Xi_-\\-U_{\Xi_-}
\end{array}
\end{array}\right]^\top\!\!
\end{equation}
such that $(A,B,C,D)\in \Sigma_{\calD}$ if and only if
\beq \label{ch5:e:char N2 model}
\bbm
I\\\hline\\[-3mm]
\begin{matrix}
A^\top & C^\top\!\\
B^\top & D^\top\!
\end{matrix}
\ebm^\top\!\! 
N
\bbm
I\\\hline\\[-3mm]
\begin{matrix}
A^\top & C^\top\!\\
B^\top & D^\top\!
\end{matrix}
\ebm
\geq 0.\equiv\qquad
    \bbm
    A^\top & C^\top\!\\
    B^\top & D^\top\!
    \ebm \in \calZ_{n + m}(N).
\eeq
% This can be restated equivalently as
% $$
% \bbm
% A^\top & C^\top\!\\
% B^\top & D^\top\!
% \ebm \in \calZ_{n + m}(N).
% $$
We can now do a similar analysis to solve the problem.
\begin{definition}[\cite{vanwaarde2023informativity}, Informativity for dissipativity of noisy data]\label{ch5:def:info diss noisy}
The noisy input-state-output data $(U_-,X,Y_-)$ are \emph{informative for dissipativity\/} with respect to the supply rate \eqref{ch5:e:supply} if there exists a matrix $P\geq 0$ such that the LMI \eqref{ch5:eq:KY_- P} holds for all systems $(A,B,C,D)\in\Sigma_{\calD}$. 
\end{definition}
% Note that, we will require that all systems consistent with the data are dissipative with a {\em common storage function}.
% Similar to the noiseless case as studied before, in the remainder of this section we will assume that the matrix $S$ representing the supply rate satisfies the inertia condition $\In(S)=(p,0,m)$.

% The following preliminary lemma states that also in the context of noisy data, the rank condition \eqref{ch5:e:full row rank} on the input-state data is necessary for informativity.

% \begin{lemma}[\cite{vanwaarde2023informativity}, Necessity of full row rank condition] \label{ch5:lem:necc noisy case} 
% Assume that $\In(S)=(p,0,m)$. If the data $(U_- ,X,Y_- )$ are informative for dissipativity with respect to the supply rate \eqref{ch5:e:supply} then \eqref{ch5:e:full row rank} holds.
% \end{lemma}

% In addition, we need the following lemma which states that if the data are informative for dissipativity with all systems in $\Sigma_{\calD}$ having a given common storage function $P \geq 0$, then $P$ is necessarily \emph{positive definite}. This is true under the additional assumption that . Combining this with the fact that $N \in \bpi_{n+p, n+m}$ as was already established above, this implies that the set $\Sigma_{\calD}$ has a nonempty interior.
% Now, to resolve the problem of system identification using this data, we will 
\begin{lemma}[\cite{9781292}, Necessity of positive definite storage]
\label{ch5:lem:P>0}
In addition to the assumption on the inertia of $S$, we also need the Schur complement $N \schur N_{22} $ to be positive definite, then the common storage function $P$ is necessarily positive	definite.
\end{lemma}

% Our next step is to partition 
% \begin{equation} \label{ch5:eq:partitionS}
% S=\bbm F & G\\G^\top & H\ebm,
% \end{equation}
% where  $F\in\R^{m\times m}$, $G\in\R^{m\times p}$, $H\in\R^{p\times p}$. For any $P \geq 0$ define
% \begin{equation} \label{ch5:eq:partitionM}
% M:=\bbm
% P & 0 & 0 & 0\\
% 0 & F & 0 & G\\
% 0 & 0 & -P & 0\\
% 0 & G^\top & 0 & H
% \ebm.
% \end{equation}
% Then the system $(A,B,C,D)$ can be seen to satisfy the dissipation inequality \eqref{ch5:eq:KY_- P} if and only if  
% \begin{equation}
% \label{ch5:eq:eqM1}
% \sysone^\top\!\! M \sysone \geq 0
% \end{equation} 
% Moreover, with this notation in place, the problem of characterizing informativity for dissipativity is equivalent to finding conditions  for the existence of a matrix $P > 0$ such that the inequality \eqref{ch5:eq:eqM1}
% %\begin{equation}
% %\label{ch5:eq:eqM1}
% %\sysone^\top\!\! M \sysone \geq 0
% %\end{equation} 
% holds for all $(A,B,C,D)$ satisfying the inequality \eqref{ch5:e:char N2 model}.
% %\begin{equation}
% %\label{ch5:eq:eqN1}
% %\systwo^\top\!\! N \systwo \geq 0 \; .
% %\end{equation}
We invoke the non-strict matrix S-lemma to solve such problem,
% Our strategy to solve this problem is to invoke the nonstrict matrix S-lemma.
\begin{theorem}[\cite{9781292}, Matrix S-lemma]
\label{t:nonstrictS-lemma}
For $M,N \in \mathbb{S}^{q+r}$, if there exists a real $\alpha \geq 0$ such that $M - \alpha N \geq 0$ then $\calZ_r(N) \subseteq \calZ_r(M)$.
Also, assuming $N \in \bpi_{q,r}$ and $N$ has at least one positive eigenvalue, then $\calZ_r(N) \subseteq \calZ_r(M)$ if and only if there exists a real $\alpha \geq 0$ such that $M-\alpha N \geq 0$. 
\end{theorem}
% Similar to the `standard' S-lemma we note that the matrix S-lemma requires $N$ to have at least one 
% 
% positive eigenvalue, an assumption known as the \emph{Slater condition}.
% 
To apply this lemma, an additional dualization result is needed as terms of \eqref{ch5:eq:eqM1} and \eqref{ch5:e:char N2 model} are \emph{transpose} of each other. 
\begin{lemma}[\cite{9781292}, Dualization of dissipation inequality]\label{ch5:lem:diss dual}
For  $P > 0$, define
\begin{equation}
\label{ch5:eq:Shat}
 \hat{S}:=\begin{bmatrix} 0&-I_p\\
 I_m&0 \end{bmatrix} S\inv \begin{bmatrix} 0&-I_m\\I_p&0 \end{bmatrix}.
\end{equation}
Then we have
\begin{equation}
\label{ch5:eq:eqL1}
\begin{bmatrix}
I&0\\
A&B
\end{bmatrix}^\top\!\!\begin{bmatrix}
P&0\\
0&-P
\end{bmatrix}\begin{bmatrix}
I&0\\
A&B
\end{bmatrix}+\begin{bmatrix}
0&I\\
C&D
\end{bmatrix}^\top\!\! S \begin{bmatrix}
0&I\\
C&D
\end{bmatrix}\geq 0
\end{equation}
if and only if 
\begin{align}\label{ch5:eq:L2}
\begin{bmatrix}
I&0 \!\\
A^\top&C^\top\!
\end{bmatrix}^\top\!\!\!\begin{bmatrix}
P^{-1}&0\\
0&-P^{-1}
\end{bmatrix}\!\!\!
\begin{bmatrix}
I&0\!\! \\
A^\top&C^\top\!\!
\end{bmatrix}\!\!+\!\!\begin{bmatrix}
0&I \!\! \\
B^\top&D^\top \!\!
\end{bmatrix}^\top\!\!\!\! \hat{S}\!\! \begin{bmatrix}
0&I \!\! \\
B^\top&D^\top \!\!
\end{bmatrix} \!\!\geq\! 0.
\end{align}
\end{lemma}
Intuitively, it connects the storage functions of the dual systems using an inverse relationship.
% Lemma~\ref{ch5:lem:diss dual} can be interpreted as saying that the system defined by the quadruple $(A,B,C,D)$ is dissipative with respect to the supply rate $S$, with storage function $P$ if and only if the dual system $(A^\top,C^\top,B^\top,D^\top)$ is dissipative with respect to the supply rate $\hat{S}$, with storage function $P^{-1}$.% A behavioral analogue of this result was obtained in \cite{Willems2002}, Proposition 12. 

Now, we will do similar partitioning procedure for this dual system,
$$
-S^{-1} = \begin{bmatrix}
\hatF&\hatG\\
\hatG^\top  &\hatH
\end{bmatrix}\qquad\text{where $\hatF=\hatF^\top \in\R^{m\times m}$, $\hatG\in\R^{m\times p}$, and $\hatH=\hatH^\top \in\R^{p\times p}$ }
$$
and define 
\begin{equation} \label{ch5:eq:Mhat}
\hat{M} := \begin{bmatrix}
    P\inv & 0 & 0 & 0 \\
    0 & \hatH & 0 &-\hatG^\top\\
    0 & 0 & -P\inv & 0 \\
    0 & -\hatG & 0 & \hatF
    \end{bmatrix}.
\end{equation}
Then it is easily seen that $(A^\top, C^\top,B^\top, D^\top)$ satisfies the inequality \eqref{ch5:eq:L2} if and only if 
\begin{equation}
\label{ch5:eq:M1}
\bbm
I\\\hline\\[-3mm]
\begin{matrix}
A^\top & C^\top\!\\
B^\top & D^\top\!
\end{matrix}
\ebm^\top\!\! \hat{M} \bbm
I\\\hline\\[-3mm]
\begin{matrix}
A^\top & C^\top\!\\
B^\top & D^\top\!
\end{matrix}
\ebm \geq 0.
\end{equation} 
% We may now observe that, under the assumptions that $\In(S)=(p,0,m)$ and schure complement of block $N_{22}$ is $>0$, informativity for dissipativity with respect to the supply rate given by $S$ holds if and only if there exists $P>0$ such that the quadratic inequality \eqref{ch5:eq:M1} holds for all $(A,B,C,D)$ that satisfy the the quadratic inequality \eqref{ch5:e:char N2 model}, equivalently 
% \begin{equation} \label{ch5:eq:Z-inclusion}
% \calZ_{n +m}(N) \subseteq \calZ_{n+m}(\hat{M}).
% \end{equation}
% This brings us in position to apply Theorem \ref{t:nonstrictS-lemma} and to obtain the following characterization for informativity for dissipativity for noisy input-state-output data.
\bthe[\cite{vanwaarde2023informativity}, Informativity for dissipativity of noisy data] \label{ch5:t:noise 1}
The data $(U_-,X,Y_-)$ with noise as stated in \ref{ch5:assumption on noise samples}. Under the assumption of, $\In(S) = (p,0,m)$ and $N \schur N_{22} >0$,
% Partition 
% \begin{equation} \label{ch5:eq:partition of Sinv}
% -S\inv = \begin{bmatrix}
% \hatF&\hatG\\
% \hatG^\top  &\hatH
% \end{bmatrix},
% \end{equation}
% where $\hatF=\hatF^\top \in\R^{m\times m}$, $\hatG\in\R^{m\times p}$, and $\hatH=\hatH^\top \in\R^{p\times p}$. 
the data is informative for dissipativity with respect to the supply rate \eqref{ch5:e:supply} if and only if there exist a real matrix $Q \in \S{n}$, $Q >0$ and a scalar $\alpha\geq 0$ such that 
\begin{equation}
\begin{bmatrix}
    \! Q & \!0\! & \!0\! & 0 \!\!\! \\
    \! 0 & \!\hatH\! & \!0\! &-\hatG^\top \!\!\! \\
   \! 0 & \!0\! & \!-Q\! & 0 \!\!\! \\
   \! 0 & \!-\hatG\! & \!0\! & \hatF \!\!\!
    \end{bmatrix} \!-\! \alpha\!\!
    \None
     \!\!\!\!\geq\! 0. \label{ch5:eq:LMI2}
\end{equation}
The common storage function is given by $P : = Q^{-1}$.
\ethe
Since only the last block row in \eqref{eq:extended_sys_bw} is uncertain, 
we introduce the notation
\begin{align}
\begin{split}
\label{eq:extended_sys2}
\xi_{k+1} &= 
\begin{bmatrix} 
\widetilde{A}_1 \\
\widetilde{C} \end{bmatrix} 
\xi_k + \begin{bmatrix} \widetilde{B}_1 \\ \widetilde{D} \end{bmatrix} u_{k} + \begin{bmatrix} 0 \\  b_v \end{bmatrix} v_{k}, \\
y_k &= \widetilde{C} \xi_k + \widetilde{D} u_k + b_v v_{k},
\end{split} 
\end{align}
where $\widetilde{A}_1 \in \mathbb{R}^{((p+m)l-p) \times (p+m)l}$ and $\widetilde{B}_1 \in \mathbb{R}^{((p+m)l-p) \times m}$ are known (cf.~\eqref{eq:extended_sys_bw}), and $\widetilde{A}_2 \in \mathbb{R}^{p \times (p+m)l}$ and $\widetilde{D} \in \mathbb{R}^{p \times m}$ are unknown. 

With the input-output viewpoint~\eqref{eq:sys_diff} and the extended state system representation~\eqref{eq:extended_sys_bw}, we can follow a similar approach as in Sec.~\ref{sec:noise} to derive dissipativity conditions based on noisy input-output data.
Similar as in the input-state case, we denote the actual noise sequence which yields the available input-output trajectory $\{u_k\}_{k=0}^{N-1}$, $\{y_k\}_{k=0}^{N-1}$ by $\{\hat{v}_k\}_{k=0}^{N-1}$. While the exact noise instance is unknown, we assume that we have information on a bound on the stacked matrix
\begin{align}
\hat{V} = \begin{bmatrix} \hat{v}_{l} & \hat{v}_{l+1}  & \dots & \hat{v}_{N-1} \end{bmatrix}
\label{eq:vhat}
\end{align}
as specified in the following assumption.
\begin{assumption}
\label{as:V}
The matrix $\hat{V}$ in \eqref{eq:vhat} is an element of the set
\begin{align*}
\mathcal{V} = \{ V \in \mathbb{R}^{p \times (N-l)} | 
\begin{bmatrix} V^\top \\ I \end{bmatrix}^\top \begin{bmatrix} Q_v & S_v \\ S_v^\top & R_v \end{bmatrix} \begin{bmatrix} V^\top \\ I \end{bmatrix} \succeq 0 \},
\end{align*}
where $Q_v \in \mathbb{R}^{(N-l) \times (N-l)}$, $S_v \in \mathbb{R}^{(N-l) \times m_v}$ and $R_v \in \mathbb{R}^{m_v \times m_v}$ with $Q_v \prec 0$. 
\end{assumption}
Due to the presence of noise, there generally exist multiple matrix pairs $(\widetilde{C}, \widetilde{D})$ which are consistent with the data for some noise sequence $V \in \mathcal{V}$. We denote the set of all such matrix pairs consistent with the input-output data and the noise bound by 
\begin{align*}
\Sigma_{U,Y} = \{ (\widetilde{A}_{2,\text{d}}, \widetilde{D}_{\text{d}}) | Y_\Xi = \widetilde{A}_{2,\text{d}} \Xi + \widetilde{D}_{\text{d}} U_\Xi + b_v V, V \in \mathcal{V} \}.
\end{align*}
Along the lines of Sec.~\ref{sec:noise}, this leads to an equivalent formulation for the set $\Sigma_{U,Y}$.
\begin{lemma}
\label{lem:para2}
It holds that
\begin{align}
\Sigma_{U,Y} = \{ (\widetilde{A}_{2,\text{d}}, \widetilde{D}_{\text{d}}) | 
\begin{bmatrix} \widetilde{A}_{2,\text{d}}^\top \\ \widetilde{D}_{\text{d}}^\top \\ I \end{bmatrix}^\top \begin{bmatrix} \bar{Q}_v & \bar{S}_v  \\ \bar{S}_v^\top & \bar{R}_v \end{bmatrix} \begin{bmatrix} \widetilde{A}_{2,\text{d}}^\top \\ \widetilde{D}_{\text{d}}^\top \\ I \end{bmatrix} \succeq 0
\}
\label{eq:At2Dt}
\end{align}
with 
\begin{align*}
\bar{Q}_{v} &= \begin{bmatrix} \Xi \\ U_\Xi\end{bmatrix} Q_v \begin{bmatrix} \Xi \\ U_\Xi\end{bmatrix}^\top, \\
\bar{S}_{v} &= -\begin{bmatrix} \Xi \\ U_\Xi\end{bmatrix} \left( Q_v Y_\Xi^\top + S_v b_v^\top \right), \\
\bar{R}_{v} &= Y_\Xi Q_v Y_\Xi^\top + Y_\Xi S_v b_v^\top + b_v S_v^\top Y_\Xi^\top + b_v R_v b_v^\top.
\end{align*}
\end{lemma}
\begin{proof}
With
\begin{align*}
\begin{bmatrix} V^\top \\ I \end{bmatrix} = \begin{bmatrix}- \Xi^\top & -U_\Xi^\top &  Y_\Xi^\top \\ 0 & 0 & I \end{bmatrix} \begin{bmatrix} \widetilde{A}_{2,\text{d}}^\top \\ \widetilde{D}_{\text{d}}^\top  \\ I \end{bmatrix},
\end{align*}
this lemma can be proven similar to the proof of \cite[Lem.~4 and Rem.~2]{vanWaarde2022b}.
\end{proof}

With the quadratic bound on the unknown matrices $( \widetilde{A}_{2, \text{d}} , \widetilde{D}_{\text{d}})$ from an input-output trajectory, we can again reformulate the uncertain system as a linear fractional transformation (LFT) of a nominal system 
\begin{align}
\begin{split}
\begin{bmatrix}
\xi_{k+1} \\ \widetilde{z}_k
\end{bmatrix}
= 
\begin{bmatrix}
\begin{bmatrix}
\widetilde{A}_1 & \widetilde{B}_1 & 0 \\
0 & 0  & I 
\end{bmatrix}\\
\begin{bmatrix}
\phantom{_1}I\phantom{_1} & \phantom{_1}0\phantom{_1} & 0 \\
0 & I & 0 
\end{bmatrix}
\end{bmatrix}
\begin{bmatrix}
\begin{bmatrix} \xi_k \\ u_k \end{bmatrix} \\ \widetilde{v}_k
\end{bmatrix} \\
\widetilde{v}_k = \begin{bmatrix} \widetilde{A}_{2, \text{d}} & \widetilde{D}_{\text{d}} \end{bmatrix} \widetilde{z}_k,
\end{split}
\label{eq:lft2}
\end{align}
with $( \widetilde{A}_{2, \text{d}} , \widetilde{D}_{\text{d}}) \in \Sigma_{U,Y}$. This allows us to again apply robust analysis results to guarantee dissipativity properties from noisy input-output trajectories. For this, 
we assume again that the inverse \eqref{eq:inv3} exists.
%
\begin{theorem}\label{thm:noise_io}
Let $\widetilde{R} \preceq 0$. If there exists a matrix $P =P^\top \succ 0$, $\tau > 0$ such that \eqref{eq:rob_io_dual} holds,
then~\eqref{eq:sys_diff} is $(Q,S,R)$-dissipative for all matrices consistent with the data $( \widetilde{A}_{2, \text{d}} , \widetilde{D}_{\text{d}}) \in \Sigma_{U,Y}$.
\end{theorem}
\begin{proof}
With the result from Lem.~\ref{lem:para2}, the proof follows along the arguments of the proof of Thm.~\ref{thm:noise}.
\end{proof}
%

\begin{align}\label{eq:rob_io_dual}
% \setstretch{1.25}
\left(
\begin{bmatrix}
\begin{bmatrix}\widetilde{A}_1^\top & 0\end{bmatrix} & 0 & \begin{bmatrix}I & 0\end{bmatrix} \\ 
-I & 0 & 0 \\
\begin{bmatrix} \widetilde{B}_1^\top & 0 \end{bmatrix} & 0 & \begin{bmatrix} 0 & I \end{bmatrix} \\ 
0 & -I & 0 \\
0 & 0 & I \\ 
\begin{bmatrix} 0&I \end{bmatrix} & I & 0 \\
\end{bmatrix}
\right)^\top
% \mleft(
\begin{bmatrix}
-P&0&0&0&0&0\\
0&P&0&0&0&0\\\hline
0&0&-\widetilde{R}&-\widetilde{S}^{\top}&0&0\\
0&0&-\widetilde{S}&-\widetilde{Q}&0&0\\\hline
0&0&0&0& -\tau \bar{Q}_{v}&-\tau \bar{S}_{v} \\
0&0&0&0&-\tau \bar{S}_{v}^\top & -\tau \bar{R}_{v}
\end{bmatrix}
\begin{bmatrix}
\begin{bmatrix}\widetilde{A}_1^\top & 0\end{bmatrix} & 0 & \begin{bmatrix}I & 0\end{bmatrix} \\ 
-I & 0 & 0 \\
\begin{bmatrix} \widetilde{B}_1^\top & 0 \end{bmatrix} & 0 & \begin{bmatrix} 0 & I \end{bmatrix} \\ 
0 & -I & 0 \\
0 & 0 & I \\ 
\begin{bmatrix} 0&I \end{bmatrix} & I & 0 \\
\end{bmatrix}
\succ0
\end{align}

\end{comment}
% \begin{remark}
% Note that from the proof of Lem.~\ref{lem:extended} (and the extended system as described in~\eqref{eq:extended_sys}), one can see that the matrices $a_i$, $b_i$, $i=1, \dots, l$ \eqref{eq:sys_diff} are uniquely defined if the left inverse of $\mathcal{O}_l$ is unique and hence if
% $l = \underline{l}$ and $p \underline{l} = n$. %rendering the extended system controllable. 
% Empirical evaluations showed that in these cases, the presented approach based on the condition in \eqref{eq:rob_io_dual} worked very well in numerical examples, while for overapproximations of the lag no reasonable upper bounds on the respective dissipativity properties could be found. Improving the approach in these cases is part of future work.
% \end{remark}
%
% \label{ex:2}
% We illustrate the introduced approach with a numerical example. We consider a randomly generated system of order $n=4$ with two inputs and outputs $m=p=2$. The system has an operator gain of $\gamma_{\text{true}} = 3.73$. We assume knowledge of the lag $\underline{l}=2$ and we simulate the trajectory with $u_k$, $k=0,\dots,N$ uniformly sampled in $[-1,1]$, for different lengths $N$. We sample the noise $\hat{v}_k$ uniformly from the ball $\|\hat{v}_k\|_2 \leq \bar{v}$ for all $k=0,\dots,N$ with $\bar{v} = 0.01$, which implies a bound on the measurement noise of $\hat{V}^\top \hat{V} \preceq \bar{v}^2 (N-\underline{l}) I$ (cf. Asm.~\ref{as:V}). We then apply the result of Thm.~\ref{thm:noise_io} and solve an SDP for finding the minimal $\gamma$ such that \eqref{eq:rob_io_dual} holds with $\Pi_\gamma$ in \eqref{eq:pi}. The resulting upper bound on the operator gain, which is guaranteed for all systems that are consistent with the data, is depicted in Fig.~\ref{fig:ex2} for different data lengths.
\begin{comment}

\section{Linear Quadratic Regulator Problem}
Consider the discrete time linear system 
\begin{equation} \label{ch4:e:disc}
\bmx(t+1) = A \bmx(t) + B \bmu(t),
\end{equation}
where $A$  and $B$ are matrices of dimensions $n \times n$ and $n \times m$, and where 
$\bmx$ is the $n$-dimensional state and $\bmu$ the $m$-dimensional input. In the linear quadratic regulator problem we quantify the performance of the system using a quadratic cost functional $J(x_0,\bmu)$ involving the state trajectory $\bmx$ and the input $\bmu$. The optimal linear quadratic regulator problem is then the problem of finding, for each initial state $x_0$ of the system, an optimal input, i.e. an input that minimizes the cost functional. In this sidebar the basics of discrete-time linear quadratic optimal control are reviewed. In the sequel,  the abbreviation `LQR'  will be used for  `linear quadratic regulator'. 

For an initial state $x_0$, let $\bmx_{x_0,\bmu}$ be the state sequence of \eqref{ch4:e:disc} resulting from the input $\bmu$ and initial condition $\bmx(0) = x_0$. We omit the subscript and simply write $\bmx$ whenever the dependence on $x_0$ and $\bmu$ is clear from the context. 

Associated to system \eqref{ch4:e:disc}, we define the quadratic cost functional
\begin{equation}\label{ch4:e:cost}
J(x_0,\bmu)=\sum_{t=0}^\infty  \bmx^\top(t) Q \bmx(t) + \bmu^\top(t) R \bmu(t),
\end{equation}
where $Q \in \S{n}$ is positive semidefinite and $R \in \S{m}$ is positive definite. Then, the optimal LQR problem is the following: 
\begin{problem}[The LQR problem]
	Determine for every initial condition $x_0$ an input $\bmu^*$, such that $\lim_{t\to\infty} \bmx_{x_0,\bmu^*}(t) = 0$, and the cost functional $J(x_0,\bmu)$ is minimized under this constraint. 
\end{problem}
\noindent Such an input $\bmu^*$ is called optimal for the given $x_0$. Of course, an optimal input does not necessarily exist for all $x_0$. We say that the optimal LQR problem is {\em solvable\/} for $(A,B,Q,R)$ if for every $x_0$ there exists an input ${\bmu}^*$ such that
\begin{enumerate}
	\item The cost $J(x_0,\bmu^*)$ is finite.
	\item The limit $\lim_{t\to\infty}\bmx_{x_0,\bmu^*}(t)=0$. 
	\item The input $\bmu^*$ minimizes the cost functional, i.e., 
	\[J(x_0,{\bmu}^*)\leq J(x_0,\bar{\bmu})\]
	for all $\bar{\bmu}$ such that $\lim_{t\to\infty}\bmx_{x_0,\bar{\bmu}}(t)=0$.
\end{enumerate}
In the sequel, we will require the notion of observable eigenvalues. 
%Recall from e.g. \cite[Sec. 3.5]{Trentelman2001} that 
An eigenvalue $\lambda$ of $A$ is called $(Q,A)$-observable if 
\[ 
\rank \begin{bmatrix} A-\lambda I \\ Q \end{bmatrix}=n.
\] 
The following theorem provides necessary and sufficient conditions for the solvability of the optimal LQR problem for $(A,B,Q,R)$. 
This theorem is the discrete-time analogue to the continuous-time case.
\begin{theorem}[Conditions for LQR]\label{ch4:t:Harry}
	Let $Q \geq 0$ and $R >0$. Then the following statements hold:	
	\begin{enumerate}
		\item If $(A,B)$ is stabilizable, there exists a unique largest real symmetric solution $P^+$ to the discrete-time algebraic Riccati equation (DARE) 
		\begin{equation}
		\label{ch4:dare}
		P = A^\top PA-A^\top PB(R+B^\top P B)\inv B^\top  P A+Q,
		\end{equation}
		in the sense that $P^+ \geq P$ for every real symmetric $P$ satisfying \eqref{ch4:dare}. The matrix $P^+$ is positive semidefinite.
		\item If, in addition to stabilizability of $(A,B)$, every eigenvalue of $A$ on the unit circle is $(Q,A)$-observable then for every $x_0$ a unique optimal input $\bmu^*$ exists. Furthermore, this input sequence is generated by the feedback law $\bmu = K \bmx$, where
		\begin{equation}
		\label{ch4:optgain}
		K := -(R+B^\top P^+ B)\inv B^\top  P^+ A.
		\end{equation}
		Moreover, the matrix $A+BK$ is stable. 
		\item In fact, the optimal LQR problem is solvable for $(A,B,Q,R)$ if and only if $(A,B)$ is stabilizable and every eigenvalue of $A$ on the unit circle is $(Q,A)$-observable. 
	\end{enumerate}
\end{theorem}

If the optimal LQR problem is solvable for $(A,B,Q,R)$, we say that the matrix $K$ given by \eqref{ch4:optgain} is the optimal feedback gain for $(A,B,Q,R)$. 
An important classical control design problem is the optimal linear quadratic regulator (LQR) problem. In this subsection we will study the data-driven version of this problem within the informativity framework.  

For given state and input dimensions $n$ and $m$, again consider the model class $\mathcal{M}$ of all discrete-time linear input-state systems \eqref{ch2:e: is-system}.
%\begin{equation} \label{ch4:e:discintext}
%\bmx(t+1) = A\bmx(t) + B \bmu(t).
%\end{equation}
Assume we have input-state data on multiple time intervals, leading to data $\calD:= (U_-,X)$ as given in \eqref{ch2:eq: UXdata}. As before, the set $\Sigma_{\calD}$ of all systems in $\calM$ that are consistent with the data is then given by \eqref{ch2:eq:SigmaD}.
%equal to 
%	\begin{equation} 
%	\label{ch4:eq: SigmaD}
%	\Sigma_\calD = \left\{ (A,B) \in \calM\mid X_+= \bbm A&B \ebm
%	\begin{bmatrix}
%	X_-\\U_-
%	\end{bmatrix} \right\}.
%	\end{equation}
We assume that the data are generated by the true (but unknown) system $(A_s,B_s)$, which is therefore in $\Sigma_\calD$ itself.

In the context of the optimal LQR problem the control objective $\calO$ is: `the system must be  controlled using the optimal feedback gain'.  In order to formalize this, we introduce the following notation.
For any given $K$, let $\Sigma^{Q,R}_{K}$ denote the set of all systems of the form \eqref{ch2:e: is-system} for which $K$ is the optimal feedback gain corresponding to $Q$ and $R$, that is,
\[ 
\Sigma_K^{Q,R}:=\set{(A,B) \in \calM }{K \text{ is optimal for }(A,B,Q,R)}.
\]
This gives rise to yet another notion of informativity in line with Definition~\ref{ch1:def:par informativity}. Indeed, informativity requires the existence of a single feedback gain that is optimal for all systems consistent with the data. For the definition of solvability of the optimal LQR problem we refer to the sidebar `The linear quadratic regulator problem`.
\begin{definition}[Informativity for LQR]
	Given matrices $Q$ and $R$, we say that the data $\calD = (U_-,X)$ are \emph{informative for optimal linear quadratic regulation} if the optimal LQR problem is solvable for all $(A,B) \in \Sigma_{\calD} $ and there exists $K$ such that $\Sigma_{\calD} \subseteq \Sigma^{Q,R}_{K}$.
\end{definition}
An instrumental result in obtaining necessary and sufficient conditions for informativity for optimal linear quadratic regulation is the following lemma.
\begin{lemma}[Common solution of the Riccati equation]\label{ch4:l: same P works for all}
Let $Q=Q^\top$ be positive semidefinite and $R=R^\top$ be positive definite. Suppose the data $(U_-,X)$ are informative for optimal linear quadratic regulation. Let $K$ be such that $\Sigma_{\calD} \subseteq \Sigma^{Q,R}_{K}$. Then, there exist a square matrix $M$ and a positive semidefinite matrix $P^+$ such that for all $(A,B)\in\Sigma_{\calD}$
\begin{align}
&\!\!\!\!\!M=A+BK,\label{ch4:e: define M}\\
&\!\!\!\!\!P^+\!= A^\top\! P^+\!A\! -\! A^\top\! P^+\!B(R + B^\top\! P^+\! B)\inv B^\top\!  P^+\! A + Q,\!\!\label{ch4:e:dare aux}\\
&\!\!\!\!\!P^+-M^\top P^+M=K^\top RK+Q,\label{ch4:e:lyap aux}\\
&\!\!\!\!\!K=-(R+B^\top P^+ B)\inv B^\top  P^+ A.\label{ch4:e:uni K aux}
\end{align}
\end{lemma}
Statement \eqref{ch4:e:dare aux} of the lemma says that if the data are informative, there exists a common solution $P^+ \geq 0$ to the whole collection of AREs associated with systems $(A,B)$ that are consistent with the data. Statement \eqref{ch4:e:uni K aux} 
says that if $K$ is the common optimal gain for all systems that are consistent with the data, then it must be of the expected form \eqref{ch4:e:uni K aux} for all  $(A,B)$ consistent with the data. According to \eqref{ch4:e: define M}, the optimal closed loop system matrices $A + BK$ are identical for all consistent pairs $(A,B)$.

The following theorem gives necessary and sufficient conditions for informativity for optimal linear quadratic regulation. 
\begin{theorem}[Conditions for informativity \cite{8960476}]\label{ch4:t:LQinform}
	Let $Q \geq 0$ and $R > 0$. Then the data $(U_-,X)$ are informative for optimal linear quadratic regulation if and only if at least one of the following two conditions hold:
	\begin{enumerate}
		\item\label{ch4:cond:a} The data $(U_-,X)$ are informative for identification, that is, $\Sigma_{\calD}=\pset{(A_s,B_s)}$, and the optimal LQR problem is solvable for $(A_s,B_s,Q,R)$. In this case, the optimal feedback gain $K$ is of the form \eqref{ch4:e:uni K aux} where $P^+$ is the largest real symmetric solution to \eqref{ch4:e:dare aux} with $A = A_s$ and $B = B_s$.
		\item\label{ch4:cond:b} For all $(A,B) \in \Sigma_{\calD}$ we have $A=A_s$. Moreover, $A_s$ is stable, $QA_s = 0$, and the optimal feedback gain is given by $K = 0$. 
	\end{enumerate}
\end{theorem}

This theorem should be interpreted as follows. Condition \ref{ch4:cond:b}) of Theorem \ref{ch4:t:LQinform} can be considered as a  pathological case in which the only $A$ consistent with the data is the true one, namely $A_s$. This matrix $A_s$ is stable and $QA_s = 0$. Since $\bmx(t) \in \im A_s$ for all $t > 0$, we have $Q \bmx(t) = 0$ for all $t > 0$ if the input function is chosen as $\bmu = 0$. Additionally, since $A_s$ is stable, this shows that the optimal input is equal to $\bmu^* = 0$. If we set aside the pathological case \ref{ch4:cond:b}), the main message of Theorem \ref{ch4:t:LQinform} is the following: if the data are informative for optimal linear quadratic regulation they are also informative for system identification, in the sense that the set of systems consistent with the data contains only one element, i.e., $\Sigma_{\calD} = \{(A_s,B_s) \}$.
%This observation is consistent with the paper \cite{Polderman1986} that showed the necessity of identifiability of the true system in adaptive LQ control.

% At first sight, this might seem like a negative result in the sense that data-driven LQR is only possible with data that are also informative enough to uniquely identify the system. However, at the same time, Theorem \ref{ch4:t:LQinform} can be viewed as a positive result in the sense that it provides fundamental justification for the data conditions imposed in e.g. \cite{DePersis2020}. Indeed, in \cite{DePersis2020} the data-driven infinite horizon LQR problem\footnote{Note that the authors of \cite{DePersis2020} formulate this problem as the minimization of the $H_2$-norm of a certain transfer matrix.} is solved using input-state data under the assumption that the input is persistently exciting of sufficiently high order. Under the latter assumption, the input-state data are informative for system identification, i.e., the matrices $A_s$ and $B_s$ can be uniquely determined from data. Theorem \ref{ch4:t:LQinform} justifies such a strong assumption on the richness of data in data-driven linear quadratic regulation.
%
%
% The data-driven \emph{finite} horizon LQR problem was solved under a persistency of excitation assumption in \cite{Markovsky2007}. Our results suggest that also in this case informativity for system identification is necessary for data-driven LQR, although further analysis is required to prove this claim.
%\end{remark}

Although Theorem \ref{ch4:t:LQinform} gives necessary and sufficient conditions under which the data are informative for optimal linear quadratic regulation, it might not be directly clear how these conditions can be verified given the input-state data. Therefore, in what follows we rephrase the conditions of Theorem \ref{ch4:t:LQinform} in terms of the data matrices $X$ and $U_-$.


\begin{theorem}[Alternative conditions for informativity \cite{8960476}]
	\label{ch4:t:LQinform2}
	Let $Q \geq 0$ and $R >0$. Then the data $(U_-,X)$ are informative for optimal linear quadratic regulation if and only if at least one of the following two conditions hold:
	\begin{enumerate}
		\item\label{ch4:cond:a2} The data $(U_-,X)$ are informative for identification, equivalently, there exists $\begin{bmatrix} V_1 & V_2\end{bmatrix}$ such that 
\begin{equation}\label{ch2:eq:V1 V2} \begin{bmatrix} X_- \\ U_- \end{bmatrix} \begin{bmatrix} 	V_1 & V_2	\end{bmatrix} = \begin{bmatrix} I_n & 0 \\ 0& I_m\end{bmatrix}. \end{equation}		
Moreover, the optimal LQR problem is solvable for $(A_s,B_s,Q,R)$, where $A_s=X_+V_1$ and $B_s=X_+V_2$.
		\item\label{ch4:cond:b2} There exists $\Theta \in \mathbb{R}^{T \times n}$ such that $X_- \Theta = (X_- \Theta)^\top$, 	$U_- \Theta = 0 $, 
		\begin{equation}
		\label{ch4:e:LMI/E/K/Q}
		\begin{bmatrix}
		X_- \Theta & X_+ \Theta \\ \Theta^\top X_+^\top & X_- \Theta
		\end{bmatrix} > 0.
		\end{equation}
		and $ QX_+\Theta = 0$. 
	\end{enumerate}
\end{theorem}

It is also possible to directly compute the optimal LQR feedback gain $K$ from the given data. 
%For this, we will employ ideas from the study of Riccati inequalities (see e.g \cite{Ran1988}).
%The main idea is to replace the Riccati inequality by the linear matrix inequality $\mathcal{L}(P) \leq 0$, where
%\begin{equation}
%\label{ch3:dataineq}
%\mathcal{L}(P) := \xmt P\xm-\xpt P\xp -\xmt Q\xm-\umt R\um.
%\end{equation}
%Note that the linear operator $\mathcal{L}$ is completely defined by the data matrices $X$ and $U_-$, and the weight matrices $Q$ and $R$. 
%
Indeed, the following theorem asserts that $P^+$ as in Lemma~\ref{ch4:l: same P works for all} can be found as the unique solution to an optimization problem involving only the data. Furthermore, the optimal feedback gain $K$ can subsequently be found by solving a set of linear equations. In the sequel, for a given square matrix $M$, $\trace(M)$ will denote the trace of $M$.

\begin{theorem}[A semi-definite programming approach \cite{8960476}]
	\label{ch4:t:LQgaindata}
Let $Q \geq 0$ and $R > 0$. Suppose that the data $(U_-,X)$ are informative for optimal linear quadratic regulation. Consider the linear operator $P\mapsto\calL(P)$ defined by
$$
\mathcal{L}(P) := \xmt P\xm-\xpt P\xp -\xmt Q\xm-\umt R\um.
$$
Let $P^+$ be as in Lemma~\ref{ch4:l: same P works for all}. The following statements hold:
	\begin{enumerate}
		\item \label{ch4:semidefiniteprogram} The matrix $P^+$ is equal to the unique solution to the optimization problem
		\begin{align*}
		\text{ maximize } \: &\trace(P) \\
		\text{ subject to } \: &P  \geq 0 
		\,\,\text{ and }\,\,  \mathcal{L}(P) \leq 0.
		\end{align*} 
		\item There exists a right inverse $X_-^\sharp$ of $X_-$ such that
			\begin{align}
			\label{ch4:eq:1}
			\mathcal{L}(P^+) X_-^\sharp &= 0.
			\end{align}
		Moreover, if $X_-^\sharp$ satisfies \eqref{ch4:eq:1}, then the optimal feedback gain is given by $K = U_- X_-^\sharp$.
	\end{enumerate}
\end{theorem}
From a design viewpoint, the optimal feedback gain $K$ can be found in the following way. First solve the semidefinite program in Theorem \ref{ch4:t:LQgaindata}. Subsequently, compute a solution $X_-^\sharp$ to the linear equations $X_- X_-^\sharp = I$ and \eqref{ch4:eq:1}. Then, the optimal feedback gain is given by $K = U_- X_-^\sharp$.
\end{comment}
Now, let us discuss some vulnerabilities of data-driven control.